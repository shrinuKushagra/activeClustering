\documentclass{article}
\usepackage[final,nonatbib]{nips_2016}

\def\COMPLETE{}
\usepackage[boxruled]{algorithm2e}
\usepackage{amsmath,amssymb,amstext,amsthm}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{graphicx}
\usepackage{hyperref}

\usepackage{xr}
\externaldocument[A-]{figures/activeClustering}

\usepackage{color}
\usepackage[toc,page]{appendix}
\usepackage{xspace}
\usepackage[inline]{enumitem}
\usepackage{times}
\usepackage{float}
\usepackage{capt-of}

\usepackage{bbm}

\usepackage{tikz}
\usetikzlibrary{shapes, calc, arrows, through, intersections, decorations.pathreplacing, patterns}

\newcommand{\mc}{\mathcal}
\newcommand{\mb}{\mathbf}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\vcdim}{VC-Dim}
\DeclareMathOperator{\vol}{vol}

\renewcommand\labelitemi{$\bullet$}
\renewcommand{\labelitemii}{$\star$}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}



\newcommand{\todo}{\textcolor{blue}{[TODO]}\xspace}
\newcommand{\complete}{\textcolor{red}{[TO BE COMPLETED]}\xspace}
\newcommand{\wip}{\textcolor{red}{[Work in progress]}\xspace}
\newcommand{\multlinecomment}[1]{\directlua{-- #1}}




\newcommand{\fix}{\textcolor{blue}{[FIX]}\xspace}
\newcommand{\q}{\textcolor{blue}{[?]}\xspace}

\title{Clustering with Same-Cluster Queries}
%\institute{School of Computer Science\\University of Waterloo\\ Waterloo, ON, N2L 3G1 \\CANADA \\ \email{\{skushagr@,shai@cs.\}uwaterloo.ca}}


%%%%% Document Body %%%%%
\begin{document}
\maketitle

\appendix
\section{Relationships Between Query Models}
\label{appendix:diffQueryModels}

\begin{proposition}
Any clustering algorithm that uses only $q$ same-cluster queries can be adjusted to use $2q$ cluster-assignment queries (and no same-cluster queries) with the same order of time complexity.
\end{proposition}
\begin{proof}
We can replace each same-cluster query with two cluster-assignment queries as in $Q(x_1,x_2)={\mathbbm{1}}\{Q(x_1)=Q(x_2))\}$.
\end{proof}

\begin{proposition}
Any algorithm that uses only $q$ cluster-assignment queries can be adjusted to use $kq$ same-cluster queries (and no cluster-assignment queries) with at most a factor $k$ increase in computational complexity, where $k$ is the number of clusters.
\end{proposition}
\begin{proof}
If the clustering algorithm has access to an instance from each of $k$ clusters (say $x_i\in X_i$), then it can simply simulate the cluster-assignment query by making $k$ same-cluster queries ($Q(x) = \argmax_{i}\mathbbm{1}\{Q(x, x_i)\}$). Otherwise, assume that at the time of querying $Q(x)$ it has only instances from $k^\prime<k$ clusters. In this case, the algorithm can do the same with the $k^\prime$ instances and if it does not find the cluster, assign $x$ to a new cluster index. This will work, because in the clustering task the output of the algorithm is a partition of the elements, and therefore the indices of the clusters do not matter.
\end{proof}


%-----------------------------------------------------------
%-----------------------------------------------------------
%-----------------------------------------------------------


\section{Comparison of $\gamma$-Margin and $\alpha$-Center Proximity}
\label{appendix:gammaMrginVsAlphaCenter}

In this paper, we introduced the notion of $\gamma$-margin niceness property. We further showed upper and lower bounds on the computational complexity of clustering under this assumption. It is therefore important to compare this notion with other previously-studied clusterability notions.

%, and provided upper (with query) and lower bounds on $\gamma$, when the metric space $M$ is Euclidean and the centers are allowed to be points in the metric space. 

An important notion of niceness of data for clustering is $\alpha$-center proximity property.

\begin{definition}[$\alpha$-center proximity \cite{awasthi2012center}]
\label{defn:alphacp}
Let $(\mc X, d)$ be a clustering instance in some metric space $M$, and let $k$ be the number of clusters. We say that a center-based clustering $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ induced by centers $c_1, \ldots, c_k \in M$ satisfies the $\alpha$-center proximity property (with respect to  $\mc X$ and $k$) if the following holds 
$$\forall x \in C_i, i\neq j, \alpha d(x, c_i) < d(x, c_j)$$
\end{definition}


 This property has been considered in the past in various studies \cite{balcan2012clustering,awasthi2012center}. In this appendix we will show some connections between $\gamma$-margin and $\alpha$-center proximity properties.
 
 
  It is important to note that throughout this paper we considered clustering in Euclidean spaces. Furthermore, the centers were not restricted to be selected from the data points. 
  However, this is not necessarily the case in other studies.





\begin{table}[]
\centering
\caption{Known results for $\alpha$-center proximity}
\label{table:alphacp}
\begin{tabular}{lll}
\cline{2-3}
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{Euclidean} & \multicolumn{1}{l|}{General Metric} \\ \hline
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Centers \\ from data\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Upper bound : $\sqrt{2}+1$  \cite{balcan2012clustering}\\ Lower bound : ?\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Upper bound : $\sqrt{2}+1$  \cite{balcan2012clustering}\\ Lower bound : 2 \cite{ben2014data}\end{tabular}} \\ \hline
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Unrestricted \\ Centers \end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Upper bound : $2+\sqrt{3}$ \cite{awasthi2012center}\\ Lower bound : ?\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Upper bound : $2+\sqrt{3}$ \cite{awasthi2012center}\\ Lower bound : 3 \cite{awasthi2012center}\end{tabular}} \\ \hline
 &  & 
\label{table:alphacenter}
\end{tabular}
\end{table}

An overview of the known results under $\alpha$-center proximity is provided in Table \ref{table:alphacenter}. The results are provided for the case that the centers are restricted to be selected from the training set, and also the unrestricted case (where the centers can be arbitrary points from the metric space). Note that any upper bound that works for general metric spaces also works for the Euclidean space. 

We will show that using the same techniques one can prove upper and lower bounds for $\gamma$-margin property. It is important to note that for $\gamma$-margin property, in some cases the upper and lower bounds match. Hence, there is no hope to further improve those bounds unless P=NP. A summary of our results is provided in \ref{table:gammamargin}.  

\begin{table}[]
\centering
\caption{Results for $\gamma$-margin}
\label{table:gammamargin}
\begin{tabular}{lll}
\cline{2-3}
\multicolumn{1}{l|}{}                                                                     & \multicolumn{1}{l|}{Euclidean} & \multicolumn{1}{l|}{General Metric}                                                                         \\ \hline
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Centers \\ from data\end{tabular}}        & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Upper bound : 2 (Thm. \ref{thm:upperCenterData})\\ Lower bound : ? \end{tabular}}    &       \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Upper bound : 2 (Thm. \ref{thm:upperCenterData})\\ Lower bound : 2 (Thm. \ref{thm:lowerCenterData})\end{tabular}}           \\ \hline
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Unrestricted \\Centers \end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Upper bound : 3 (Thm. \ref{thm:upperCenterMetric})\\ Lower bound : 1.84 (Thm. \ref{A-thm:gammaLower})\\ \end{tabular}}         & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Upper bound : 3 (Thm. \ref{thm:upperCenterMetric})\\ Lower bound : 3 (Thm. \ref{thm:lowerCenterMetric})\\ Awasthi\end{tabular}} \\ \hline
                                                                                          &                       &    
\label{table:gammamargin}                                                                                                                                                                                                 
\end{tabular}
\end{table}

\subsection{Centers from data}
\begin{theorem}
\label{thm:upperCenterData}
Let $(X , d)$ be a clustering instance and $\gamma \ge 2$. Then, Algorithm 1 in \cite{balcan2012clustering} outputs a tree $\mc T$ with the following property: 


Any $k$-clustering $\mc C^* = \{C_1^*, \ldots, C_k^* \}$ which satisfies the $\gamma$-margin property and its cluster centers $\mu_1, \ldots, \mu_k$ are in $X$, is a pruning of the tree $T$. In other words, for every $1 \le i \le k$, there exists a node $N_i$ in the tree $T$ such that $C_i^* = N_i$.
\end{theorem}

\begin{proof}
Let $p, p' \in C_i^*$ and $q \in C_j^*$. \cite{balcan2012clustering} prove the correctness of their algorithm for $\alpha > \sqrt{2} + 1$. Their proof relies only on the following three properties which are implied when $\alpha > \sqrt{2} + 1$. We will show that these properties are implied by $\gamma > 2$ instances as well.
\begin{itemize}[nolistsep,noitemsep]
\item $d(p, \mu_i) < d(p, q)$\\
$\gamma d(p, \mu_i) < d(q, \mu_i) < d(p, q) + d(p, \mu_i) \implies d(p, \mu_i) < \frac{1}{\gamma-1}d(p, q)$.
\item $d(p, \mu_i) < d(q, \mu_i)$\\
This is trivially true since $\gamma > 2$.
\item $d(p, \mu_i) < d(p', q)$\\
Let $r = \max_{x \in C_i^*} d(x, \mu_i)$. Observe that $d(p, \mu_i) < r$. Also, $d(p', q)> d(q, \mu_i)-d(p', \mu_i) > \gamma r - r = (\gamma -1)r$.
\end{itemize}
\end{proof}

\begin{theorem}
\label{thm:lowerCenterData}
Let $(\mc X, d)$ be a clustering instance and $k$ be the number of clusters. For $\gamma < 2$, finding a $k$-clustering of $X$ which satisfies the $\gamma$-margin property and where the corresponding centers $\mu_1, \ldots, \mu_k$ belong to $\mc X$ is NP-Hard.
\end{theorem}
\begin{proof}
For $\alpha < 2$, \cite{ben2014data} proved that in general metric spaces, finding a clustering which satisfies the $\alpha$-center proximity and where the centers $\mu_1, \ldots, \mu_k \in \mc X$ is NP-Hard. Note that the reduced instance in their proof, also satisfies $\gamma$-margin for $\gamma < 2$. 
\end{proof}

\subsection{Centers from metric space}
\begin{theorem}
\label{thm:upperCenterMetric}
Let $(X , d)$ be a clustering instance and $\gamma \ge 3$. Then, the standard single-linkage algorithm outputs a tree $\mc T$ with the following property:


Any $k$-clustering $\mc C^* = \{C_1^*, \ldots, C_k^* \}$ which satisfies the $\gamma$-margin property is a pruning of $T$. In other words, for every $1 \le i \le k$, there exists a node $N_i$ in the tree $T$ such that $C_i^* = N_i$. 
\end{theorem}

\begin{proof}
\cite{balcan2008discriminative} showed that if a clustering $C^*$ has the strong stability property, then single-linkage outputs a tree with the required property. It is simple to see that if $\gamma > 3$ then instances have strong-stability and the claim follows.  
\end{proof}


\begin{theorem}
\label{thm:lowerCenterMetric}
Let $(\mc X, d)$ be a clustering instance and $\gamma < 3$. Then, finding a $k$-clustering of $X$ which satisfies the $\gamma$-margin is NP-Hard.
\end{theorem}
\begin{proof}
\cite{awasthi2012center} proved the above claim but for $\alpha < 3$ instances. Note however that the construction in their proof satisfies $\gamma$-margin for $\gamma < 3$. 
\end{proof}



%-----------------------------------------------------------
%-----------------------------------------------------------
%-----------------------------------------------------------

\section{Proofs of Lemmas \ref{A-lemma:kmeansEquivalenceX3C} and \ref{A-lemma:gammaLower}}
\label{appendix:lowerBoundProof}

In Section \ref{A-section:lowerBounds} we proved Theorem \ref{A-thm:gammaLower} based on two technical results (i.e., lemma \ref{A-lemma:kmeansEquivalenceX3C} and \ref{A-lemma:gammaLower}). In this appendix we provide the proofs for these lemmas. In order to start, we first need to establish some properties about the Euclidean embedding of $X$ proposed in Section \ref{A-section:lowerBounds}.

%$$L_1 = (6m+3)w, L_2 = 6m(l-1), L=L_1 + L_2, \alpha = \frac{d}{w}-\frac{1}{2w^3}$$

\begin{definition}[$A$- and $B$-Clustering of $R_i$]
\label{defn:abclusteringVattani}

An $A$-Clustering of row $R_i$ is a clustering in the form of $\{\{s_i\}, \{r_{i,1}, r_{i,2}\}, \{r_{i,3}, r_{i,4}\}, \ldots,$ $ \{r_{i,6m-1}, r_{i,6m}\},\{r_{i, 6m+1}, f_i\}\}$. A $B$-Clustering of row $R_i$ is a clustering in the form of $\{\{s_i, r_{i, 1}\}, \{r_{i,2}, r_{i,3}\}, \{r_{i,4}, r_{i,5}\}, \ldots,$ $ \{r_{i,6m}, r_{i,6m+1}\},\{f_i\}\}$. 
\end{definition}

\begin{definition}[Good point for a cluster]
\label{defn:goodPointVattani}
A cluster $C$ is good for a point $z \not\in C$ if adding $z$ to $C$ increases cost by exactly $\frac{2w}{3}h^2$ 
\end{definition}

Given the above definition, the following simple observations can be made. 
\begin{itemize}[nolistsep,noitemsep]
\item The clusters $\{r_{i,2j-1}, r_{i, 2j}\}$, $\{r_{i,2j}, r_{i, 2j+1}\}$ and $\{g_{i,j}\}$ are good for $x_{i,j}$ and $y_{i-1,j}$.
\item The clusters $\{r_{i,2j}, r_{i, 2j+1}\}$ and $\{g_{i,j}\}$ are good for $x_{i,j}'$ and $y_{i-1,j}'$.
\end{itemize}

\begin{definition}[Nice Clustering]
\label{defn:niceClustering}
A $k$-clusteirng is nice if every $g_{i,j}$ is a singleton cluster, each $R_i$ is grouped in the form of either an $A$-clustering or a $B$-clustering, and each point in $Z_i$ is added to a cluster which is good for it.
\end{definition}

It is straightforward to see that a row grouped in a $A$-clustering costs $(6m+3)w-\alpha$ while a row in $B$-clustering costs $(6m+3)w$. Hence, a nice clustering of $H_{l,m} \cup Z$ costs at most $L_1 + L_2$. More specifically, if $t$ rows are group in a $A$-clustering, the nice-clustering costs $L_1+L_2-t\alpha$. Also, observe that any nice clustering of $X$ has only the following four different types of clusters. \begin{enumerate}[label=(\arabic*),nolistsep,leftmargin=*]
\item Type E - $\{r_{i,2j-1}, r_{i,2j+1}\}$ \\
The cost of this cluster is $2w$ and the contribution of each location to the cost (i.e., $\frac{cost}{\#locations}$) is $ \frac{2w}{2} = w$.
\item Type F - $\{r_{i,2j-1}, r_{i, 2j}, x_{i, j}\}$ or $\{r_{i,2j-1}, r_{i, 2j}, y_{i-1, j}\}$ or $\{r_{i,2j}, r_{i, 2j+1}, x_{i, j}'\}$ or $\{r_{i,2j}, r_{i, 2j+1}, y_{i-1, j}'\}$\\
The cost of any cluster of this type is $2w(1+\frac{h^2}{3})$ and the contribution of each location to the cost is at most $\frac{2w}{9}(h^2+3)$. This is equal to $\frac{16}{9}w$ because we had set $h = \sqrt 5$.
\item Type I - $\{g_{i, j}, x_{i,j}\}$ or $\{g_{i, j}, x_{i,j}'\}$  or $\{g_{i, j}, y_{i,j}\}$  or $\{g_{i, j}, y_{i,j}'\}$\\
The cost of any cluster of this type is $\frac{2}{3}wh^2$ and the contribution to the cost of each location is $\frac{w}{3}h^2$. For our choice of $h$, the contribution is $\frac{5}{3}w$.
\item Type J - $\{s_i, r_{i,1}\}$ or $\{r_{i,6m+1}, f_i\}$\\
The cost of this cluster is $3w$ (or $3w-\alpha$) and the contribution of each location to the cost is at most $1.5w$. 
\end{enumerate}
Hence, observe that in a nice-clustering, any location contributes at most $\le \frac{16}{9}w$ to the total clustering cost. This observation will be useful in the proof of the lemma below.

\begin{lemma}
\label{lemma:costNonNice}
For large enough $w = poly(l, m)$, any non-nice clustering of $X = H_{l, m} \cup Z$ costs at least $L + \frac{w}{3}$.
\end{lemma}

\begin{proof}
We will show that any non-nice clustering $C$ of $X$ costs at least $\frac{w}{3}$ more than any nice clustering. This will prove our result. The following cases are possible.

\begin{itemize}[nolistsep,leftmargin=*]
\item $C$ contains a cluster $C_i$ of cardinality $t > 6$ (i.e., contains $t$ weighted points)\\
Observe that any $x \in C_i$ has at least $t-5$ locations at a distance greater than 4 to it, and $4$ locations at a distance at least $2$ to it. Hence, the cost of $C_i$ is at least $\frac{w}{2t}(4^2(t-5)+2^24)t = 8w(t-4)$. $C_i$ allows us to use at most $t-2$ singletons. This is because a nice clustering of these $t+(t-2)$ points uses at most $t-1$ clusters and the clustering $C$ uses  $1 + (t-2)$ clusters for these points. The cost of the nice cluster on these points is $\le \frac{16w}{9}2(t-1)$. While the non-nice clustering costs at least $8w(t-4)$. For $t \ge 6.4 \implies 8(t-4) > \frac{32}{9}(t-1)$ and the claim follows. Note that in this case the difference in cost is at least $\frac{8w}{3}$. 

\item Contains a cluster of cardinality $t = 6$\\
Simple arguments show that amongst all clusters of cardinality $6$, the following has the minimum cost. $C_i = \{r_{i, 2j-1}, r_{i, 2j}, x_{i,j}, y_{i-1, j}, r_{i, 2j+1}, r_{2j+2}\}$. The cost of this cluster is $\frac{176w}{6}$. Arguing as before, this allows us to use $4$ singletons. Hence, a nice cluster on these $10$ points costs at most $\frac{160w}{9}$. The difference of cost is at least $34w$.  

\item Contains a cluster of cardinality $t = 5$\\
Simple arguments show that amongst all clusters of cardinality $5$, the following has the minimum cost. $C_i = \{r_{i, 2j-1}, r_{i, 2j}, x_{i,j}, y_{i-1, j}, r_{i, 2j+1}\}$. The cost of this cluster is $16w$. Arguing as before, this allows us to use $3$ singletons. Hence, a nice cluster on these $8$ points costs at most $16w\frac{8}{9}$. The difference of cost is at least $\frac{16w}{9}$.  

\item Contains a cluster of cardinality $t = 4$\\
It is easy to see that amongst all clusters of cardinality $4$, the following has the minimum cost. $C_i = \{r_{i, 2j-1}, r_{i, 2j}, x_{i,j}, r_{i, 2j+1}\}$. The cost of this cluster is $11w$. Arguing as before, this allows us to use $2$ singletons. Hence, a nice cluster on these $6$ points costs at most $\frac{32w}{3}$. The difference of cost is at least $\frac{w}{3}$.

\item All the clusters have cardinality $\le 3$ \\
Observe that amongst all non-nice clusters of cardinality $3$, the following has the minimum cost. $C_i = \{r_{i, 2j-1}, r_{i, 2j}, r_{i, 2j+1}\}$. The cost of this cluster is $8w$. Arguing as before, this allows us to use at most $1$ more singleton. Hence, a nice cluster on these $4$ points costs at most $\frac{64w}{9}$. The difference of cost is at least $\frac{8w}{9}$.

It is also simple to see that any non-nice clustering of size $2$ causes an increase in cost of at least $w$.

\end{itemize}
\end{proof}

%\begin{lemma}
%\label{lemma:kmeansEquivalenceX3C}
%The set $X = H_{l,n} \cup Z$ has a $k$-clustering %of cost less or equal to $L$ if and only if there %is an exact cover for the X3C instance.
%\end{lemma}
\begin{proof}[Proof of lemma \ref{A-lemma:kmeansEquivalenceX3C}]
The proof is identical to the proof of Lemma 11 in \cite{vattani2009hardness}. Note that the parameters that we use are different with those utilized by \cite{vattani2009hardness}; however, this is not an issue, because we can invoke our lemma \ref{lemma:costNonNice} instead of the analogous result in Vattani (i.e., lemma 10 in Vattani's paper). The sketch of the proof is that based on lemma \ref{lemma:costNonNice}, only nice clusterings of $X$ cost $\le L$. On the other hand, a nice clustering corresponds to an exact 3-set cover. Therefore, if there exists a clustering of $X$ of cost $\le L$, then there is an exact 3-set cover. The other way is simpler to proof; assume that there exists an exact 3-set cover. Then, the corresponding construction of $X$ makes sure that it will be clustered \emph{nicely}, and therefore will cost $\le L$.  



 %argue why the only nice clusterings of $X$ cost $\le L$. We use Lemma \ref{lemma:costNonNice} to argue the same. Note that since the parameters are much different that those considered in Vattani's original construction, we need to argue much more carefully to prove Lemma \ref{lemma:costNonNice}. 
\end{proof}

%\begin{lemma}
%\label{lemma:gammaLower}
%Any clustering of $X = H_{l,n} \cup Z$ which has %cost$\le L$ has the $\gamma$-margin property where %$\gamma = \sqrt{\frac{17}{5}} \approx 1.84$.
%\end{lemma}
\begin{proof}[Proof of lemma \ref{A-lemma:gammaLower}]
As argued before, any nice clustering has four different types of clusters. We will calculate the minimum ratio $a_i = \frac{d(y, \mu)}{d(x, \mu)}$ for each of these clusters $C_i$ (where $x \in C_i$, $y \not\in C_i$ and $\mu$ is mean of all the points in $C_i$.) Then, the minimum $a_i$ will give the desired $\gamma$. 
\begin{enumerate}[label=(\arabic*),nolistsep,leftmargin=*]
\item For Type E clusters $a_i = h/1 = \sqrt{5}$. 
\item For Type F clusters. $a_i = \frac{\frac{\sqrt{4+16(h^2-1)}}{3}}{2h/3} = \sqrt{\frac{17}{5}} \approx 1.84$. 
\item For Type I clusters, standard calculation show that $a_i > 2$.
\item For Type J clusters $a_i = \frac{2+\frac{\sqrt{6}}{2}}{\frac{\sqrt{6}}{2}} > 2$.
\end{enumerate}

\noindent Furthurmore, $|\mc X| = (12lm + 3l -6m)w$ and $k = 6lm + 2l - 3m$. Hence for $w = $poly$(l, m)$ our hardness result holds for $k = |\mc X|^{\epsilon}$ for any $0 < \epsilon < 1$.
\end{proof}
\noindent Lemmas \ref{A-lemma:kmeansEquivalenceX3C} and \ref{A-lemma:gammaLower} complete the proof of the main result (Thm. \ref{A-thm:gammaLower}). 

%================================================
%================================================
%================================================

\section{Concentration inequalities}
\label{appendixsection:conIneq}

\begin{theorem}[Generalized Hoeffding's Inequality (e.g., \cite{ashtiani2015dimension})]
\label{thm:genHoeff}
Let $X_1, \ldots. X_n$ be i.i.d random vectors in some Hilbert space such that for all $i$, $\|X_i\|_2 \le R$ and $E[X_i] = \mu$. If $n > c\frac{\log(1/\delta)}{\epsilon^2}$, then with probability atleast $1-\delta$, we have that
$$\Big\|\mu - \frac{1}{n}\sum X_i\Big\|_2^2 \le R^2\epsilon$$ 
\end{theorem}

\bibliographystyle{alpha}
\bibliography{activeClustering}
\end{document}


