\documentclass[orivec]{llncs}
\usepackage{llncsdoc}

\def\COMPLETE{}
\usepackage[boxruled]{algorithm2e}
\usepackage{amsmath,amssymb,amstext}
\usepackage[margin=1in]{geometry}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[toc,page]{appendix}

\usepackage{color}
\usepackage[toc,page]{appendix}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{times}
\usepackage{float}
\usepackage{capt-of}

\usepackage{bbm}

\usepackage{tikz}
\usetikzlibrary{shapes, calc, arrows, through, intersections, decorations.pathreplacing, patterns}

\newcommand{\mc}{\mathcal}
\newcommand{\mb}{\mathbf}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\vcdim}{VC-Dim}
\DeclareMathOperator{\vol}{vol}

\renewcommand{\qed}{\hfill\ensuremath{\blacksquare}}
\renewcommand\labelitemi{$\bullet$}

\makeatletter  %% this is crucial
\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}%
   {-18\p@ \@plus -4\p@ \@minus -4\p@}%
   {8\p@ \@plus 4\p@ \@minus 4\p@}%     <-- this
   {\normalfont\normalsize\bfseries\boldmath
   \rightskip=\z@ \@plus 8em \pretolerance=10000}}
\makeatother   %% this is crucial
\setcounter{secnumdepth}{3}


\newcommand{\todo}{\textcolor{blue}{[TODO]}\xspace}
\newcommand{\complete}{\textcolor{red}{[TO BE COMPLETED]}\xspace}
\newcommand{\wip}{\textcolor{red}{[Work in progress]}\xspace}
\newcommand{\multlinecomment}[1]{\directlua{-- #1}}




\newcommand{\fix}{\textcolor{blue}{[FIX]}\xspace}
\newcommand{\q}{\textcolor{blue}{[?]}\xspace}

\title{Active Clustering with Oracles}
\author{Student submission}
%\institute{School of Computer Science\\University of Waterloo\\ Waterloo, ON, N2L 3G1 \\CANADA \\ \email{\{skushagr@,shai@cs.\}uwaterloo.ca}}


%%%%% Document Body %%%%%
\begin{document}
\maketitle

\begin{abstract}
We should write a good abstract here.
\end{abstract}

\section{Introduction}

Clustering can be thought as the task of organizing a set of elements into coherent subsets.... %The concrete goal of clustering depends on the task. For instance, a popular paradigm for clustering is to partition the data so as to optimize a certain objective function.

clustering is ill defined/importance of prior-knowledge or getting feedback from user

computational difficulties in clustering

niceness assumptions,....still far from being realistic

a new use direction: can feedback from user/oracle help in reducing the computational complexity of the clustering problem. In particular, making NP-hard problems easy?


\subsection{Contributions}

\subsection{Related work}
balcan active split/merge

\section{Problem Formulation}



\subsection{Clustering with Query}

Let $\mc X$ be domain set and $d:(\mc X, \mc X) \rightarrow \mc R $ be a distance function over its elements. Let $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ be a clustering (i.e., a partitioning) of $\mc X$. We say $x_1 \overset{C_{\mc X}}{\sim} x_2$ if $x_1$ and $x_2$ belong to the same cluster according to $C_{\mc X}$. We further denote by $n$ the number of instance ($|{\mc X}|$) and by $k$ the number of clusters.

We assume that an oracle $\mc O$ has a clustering $C^*_{\mc X}=\{ C^*_1, \ldots C^*_k\}$ in his mind. The clustering algorithm then tries to recover $C^*_{\mc X}$ by querying the oracle. The following notions of query are natural for clustering tasks.

\begin{definition}[Equivalence Query]
An equivalence query asks whether two instances $x_1$ and $x_2$ belong to the same cluster, i.e., 
$${\mc O}(x_1, x_2) = \left\{
	\begin{array}{ll}
		\mbox{true }  & \mbox{if } x_1 \overset{C^*_{\mc X}}{\sim} x_2   \\
		\mbox{false } & o.w. 
	\end{array}
\right. $$
\end{definition}

\begin{definition}[Membership Query]
A membership query asks the cluster index that an instance $x$ belongs to. In other words ${\mc O}(x) = i$ if and only if $x \in C^*_i$.
%$$Q(x, i) = \left\{
%	\begin{array}{ll}
%		\mbox{true }  & \mbox{if } x \in C^*_i   \\
%		\mbox{false } & o.w. 
%	\end{array}
%\right. $$
\end{definition}

While equivalence queries seem to be more natural for clustering tasks, we will see that membership queries make the analysis easier and more transparent. In appendix \ref{appendix:diffQueryModels} we show that these two notions are related, in the sense that they give relatively the same power to the clustering algorithm. In the following, we assume that the queries are in the form of equivalence queries.

An active clustering instance is determined by the tuple $(\mc X, d, C^*)$. A clustering algorithm $\mc A$ is called a $q$-solver of this instance if it can recover $C^*$ by having access to $(\mc X, d)$ and making at most $q$ queries. This algorithm is further called a polynomial $q$-solver if its time-complexity is polynomial in $|\mc X|$ and $|C^*|$.

\begin{definition}[Query Complexity]
Let $G$ be a set of clustering instances. We say $G$ admits an $O(q)$ query complexity if there exists an algorithm $\mc A$ that is a polynomial $q$-solver for every clustering instance in $G$.
\end{definition}

\subsection{Center-based Clustering with Queries}

The framework inroduced in the previous section is general and can be used in different settings. In this section, we make more assumptions about the problem setting in order to make it concrete.

We say that a clustering $\mc C_{\mc X}$ is \emph{center-based} if there exists a set of corresponding centers $\mc \mu = \{\mu_1, \ldots, \mu_k\}$ such that the elements of each cluster are closer to their center than any other center. More formally, for every element $x$ in $\mc X$ we should have $x\in C_i \Leftrightarrow i=\argmin_j d(x,\mu_j)$. A clustering instance $(\mc X, d, C^*)$ is center-based if $C^*$ is a center-based clustering of $\mc X$.

Some of the most-used clustering methods (e.g., k-means and k-median clustering) are center-based. However, clustering is genrally computationally hard in these settings. Therefore, some notions of \emph{niceness} of the clustering instance (under which clustering becomes \emph{easy}) have been considered. In the following we introduce a related notion of niceness.

\begin{definition}[$\gamma$-Margin Property]
\label{defn:alphacp}
We say that a center-based clustering instance $(\mc X, d, C^*)$ has $\gamma$-margin property if for every $C_i$ in $C^*$ the following holds

$$\forall x\in C^*_i, y \notin C^*_i, \gamma d(x, \mu_i) < d(y, \mu_i)$$

where $\mu_i$ is the center corresponding to $C_i$.

\end{definition}


%, a center-based clutering algorithm outputs a set of centers $\mu$ (or the corresponding partitioning $C_{\mc X}$ based on a criterion. This criterion can be the optimization of a certain objective fucntion (e.g., k-means objective function). In the next section, we introduce a different setting where the algorithm tries to find out the clustering that an oracle has in mind by querying the oracle. 

%\begin{definition}[$\gamma$-margin]
%\label{defn:alphacp}
%Given a clustering instance $(\mc X, d)$ from some metric space $M$ and the number of clusters $k$. We say that a clustering $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ induced by centers $\mu_1, \ldots, \mu_k \in M$ has $\gamma$-margin w.r.t $\mc X$ and $k$ if the following holds. For all $x \in C_i$ and $y \in C_j$, 
%$$\gamma d(x, \mu_i) < d(y, \mu_i)$$
%\end{definition}

In clustering literature, similar notions have been considered before. One of these notions is $\alpha$-center proximity [??????]. Comparison of our notion of margin against center proximity is orthogonal to the problem being addressed in this paper, and is provided in appendix \ref{appendix:gammaMrginVsAlphaCenter}. \footnote{We just mention here that the results (both upper and lower bounds) obtained under $\alpha$-center proximity can also be obtained under $\gamma$-margin for `reasonably small' values of $\gamma$.}

Throughout the next sections, we assume that the instaces are in a Euclidean metric space (i.e., $\mc X\subset \mathbb{R}^d$). Furthermore, we assume that the clustering instance $(\mc X, d, C^*)$ is center-based and admits a $\gamma$-margin property\footnote{Naturally, the larger the value of $\gamma$, the stronger this assumtion is.}. Finally, we assume that the centers $\mu$ corresponding to $C^*$ are the centers of mass of the corresponding clusters. In other words, $\mu_i=\frac{1}{|C_i|}\sum_{x\in C_i} x$. Note that this is the case for example when the oracle's clustering is the optimal solution to the Euclidean k-means clustering problem.

\section{An Algorithm for Clustering with Query}
\label{section:clusteringWithQuery}

In this section we provide an efficient algorithm for clustering with queries. The setting is the one described in the previous section. In particular, it is assumed that the oracle has a $\gamma$-margin center-based clustering in his mind. The space is Euclidean and the center of each cluster is the center of mass of the instances in that cluster. The algorithm makes membership queires to the oracle. 

%We will assume that the target clustering $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ induced by centers $\mu_1, \ldots, \mu_k$ has the following property. For all $i$, $\mu_i$ is the mean of all the points in cluster $C_i$, that is,
%\begin{align}
%\mu_i = \frac{1}{|C_i|}\sum_{x \in C_i} x \stepcounter{equation}\tag{P\theequation} \label{property:targetClust}
%\end{align}
%Note that the optimal solution to the euclidean $k$-means cost has this property. Our algorithm is more general and works for any target clustering which has the above property (\ref{property:targetClust}).

Intuitively, our algorithm (Algorithm  \ref{alg:steinerQueryPositive}) does the following. In the first phase, it tries to approximate the center of on of the clusters. It does this by querying points from $\mc X$ uniformly at random until it has a sufficient number of points from at least one cluster (say $C_p$). It uses the mean of these points, $\mu_p'$, to approximate the cluster center. 

In the second phase, the algorithm recovers all of the instances belonging to $C_p$. In order to do that, it first sorts all of the instances based on their distance to $\mu_p\prime$. By showing that all of the points in $C_p$ lie inside a sphere centered at $\mu_p\prime$ (which does not include points from any other cluster), it tries to find the radius of this sphere by doing binary search using membership queries. After that, the elements in $C_p$ will be located and can be removed from the data set.

The algorithm repeats this process $k$ times to recover all of the clusters.

%After finding the point in $C_p$ with the maximum distance to $\mu_p\prime$ (we call this point $b_{idx}$), it puts all of the points in the sphere

%$\mu_p'$. Then, it clusters and deletes all points whose distance is $\le d(b_{idx}, \mu_p')$. We will show that this process guarantees that (as long as $\mu_p'$ is a good approximation of the actual center $\mu_p$) and $\gamma > 1$, all the points from $C_p$ are now recovered. We then repeat this process $k$ times to recover all the clusters.

The details of our approach is stated precisely in Alg. \ref{alg:steinerQueryPositive}. Note that $\beta$ is a small constant\footnote{It corresponds to the constant appeared in generalized Hoefding inequality bound, discussed in Theorem \ref{thm:genHoeff}}. Theorem \ref{thm:steinerQueryPositive} shows that if $\gamma > 1$ then our algorithm recovers the target clustering with high probability. Next, we give bounds on the time and query complexity of our algorithm. Theorem \ref{thm:steinerQueryPositiveComplexity} shows that our approach needs $O(k\log n)$ queries and runs with time complexity $O(kn\log n)$.

\RestyleAlgo{boxruled} 
\SetAlgoNoLine
\begin{algorithm}[h]
 \KwIn{Clustering instance $\mc X$, oracle $\mc O$, the number of clusters $k$ and parameter $\delta \in (0, 1)$}
 \KwOut{A clustering $\mc C$ of the set $\mc X$}

 \vspace{0.5em} $\mc C = \{\}$\\ 
 $\mc S_{1} = \mc X$\\
 $\eta = \beta \frac{\log k + \log(1/\delta)}{(\eta-1)^4}$\\
 \For{$i = 1$ to $k$}{
 	\vspace{0.7em}\textbf{Phase 1}\\
 	$l = k \eta + 1$\;
	$Z \sim U^l[\mc S_i]$    \mbox{       } //   Selects $l$ independent points from $\mc S_i$ uniformly at random\\
	For $1 \le t \le i$,\\
             \mbox{            } $Z_t = \{x \in Z : {\mc O}(x)= t\}.$ \mbox{    } //Queries the oracle about the members of $Z$\\
    % let $Z_t \subseteq \mc Z$ be the set of points with label $i$. That is, \begin{center}$A_t = \{x \in \mc Z : x \in C_t\}.$\end{center} 
%	Choose any $Z_p$ such that $|Z_p| > \eta$.\\
$p = \argmax_t |Z_t|$\\
	$\mu_p' := \frac{1}{|Z_p|}\sum_{x \in Z_p} x$.\\

	\vspace{1.5em}\textbf{Phase 2}\\
    // We know that there exists $r_i$ such that $\forall x\in {\mc S_i}$, $x\in C_i \Leftrightarrow d(x, \mu\prime_i)< r_i$.\\ 
    // Therefore, $r_i$ can be found by simple binary search\\
    
	$\widehat{\mc S_i}$ = Sorted$(\{\mc S_i\})$ \mbox{       }// Sorts elements of $\{x: x\in \mc S_i\}$ in increasing order of $d(x, \mu_p')$.\\    
	 $r_i = $ BinarySearch$(\widehat{\mc S_i})$ \mbox{      } //This step takes up to $O(\log|{\mc S_i}|)$ queries\\


%Binary search over $\widehat{\mc S_i}$, to find an index $idx$ such that $b_{idx} \in C_p$ and $b_{idx+1} \not\in C_p$. (This step involves making queries to the oracle $\mc O$).\\ %We will later prove that $d(b_{idx}, c_p') = \max_{x \in C_p}d(x, c_p')$.
	$C_p' = \{x \in \mc S_i: d(x, \mu_p') \le r_i\}$.\\
	$S_{i+1} = S_{i}\setminus C_p'$.\\
	$\mc C = \mc C \cup \{C_p'\}$
 }
 Output $\mc C$.
 \label{alg:steinerQueryPositive}
 \caption{Algorithm for $\gamma(> 1)$-margin instances with queries}
\end{algorithm}


\begin{lemma}
\label{lemma:hasGammaMargin}
Let $(\mc X, d, C)$ be a clustering instane, where $C^*$ is center-based and satisfies $\gamma$-margin property. Let $\mu$ be the set of centers corresponding to the centers of mass of $C^*$. Let $\mu_i'$ be such that $d(\mu_i, \mu_i') \le r(C_i)\epsilon$, where $r(C_i) = \max_{x\in C_i}d(x, \mu_i)$ . Then $\gamma \ge 1 + 2\epsilon$ implies that 

$$\forall x \in C_i, \forall y \in {\mc X} \setminus C_i \Rightarrow d(x, \mu_i') < d(y, \mu_i')$$  
\end{lemma}

%\begin{lemma}
%\label{lemma:hasGammaMargin}
%Given a clustering instance $(\mc X, d)$ in some metric space $M$. Let $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ be a clustering of $\mc X$ induced by centers $\mu_1, \ldots, \mu_k \in M$ which satisfies $\gamma$-margin and $\mu_i$ is the mean of points in $C_i$. Let $\mu_i' \in M$ such that $d(\mu_i, \mu_i') \le r(C_i)\epsilon$. If $\gamma \ge 1 + 2\epsilon$, then for all $x \in C_i$ and for all $y \in C_j$
%$$d(x, \mu_i') < d(y, \mu_i')$$  
%\end{lemma}

\begin{proof}
Fix any $x \in C_i$ and $y \in C_j$. $d(x, \mu_i') \le d(x, \mu_i)+d(\mu_i, \mu_i') \le r(C_i) (1+\epsilon)$. Similarly, $d(y, \mu_i') \ge d(y, \mu_i) - d(\mu_i, \mu_i') > (\gamma -\epsilon)r(C_i)$. Combining the two, we get that $d(x, \mu_i') < \frac{1+\epsilon}{\gamma-\epsilon}d(y, \mu_i')$. 
\qed
\end{proof}

\begin{lemma}
\label{lemma:phase1}
Let the framework be as in Lemma \ref{lemma:hasGammaMargin}. Let $Z_p, C_p, \mu_p$ and $\mu_p', \eta$ be as defined is Alg. \ref{alg:steinerQueryPositive}, and $\epsilon = \frac{\gamma - 1}{2}$. If $|Z_p| > \eta$, then the probability that $d(\mu_p, \mu_p') > r(C_p)\epsilon$ is at most $\frac{\delta}{k}$.
\end{lemma}
\begin{proof}
Define a uniform distribution $U$ over $C_p$. Then $\mu_p$ is the mean of this distribution, and $\mu_p\prime$ is the empirical mean. Now, we only need to use a concentration inequality to show that the empirical means is close to the true mean. Therefore, using Thm. \ref{thm:genHoeff} from Appendix \ref{appendixsection:conIneq} completes the proof.
\qed
\end{proof}

\begin{theorem}
\label{thm:steinerQueryPositive}
Let $(\mc X, d, C)$ be a clustering instane, where $C^*$ is center-based and satisfies $\gamma$-margin property. Let $\mu$ be the set of centers corresponding to the centers of mass of $C^*$.
%Given a clustering instance $(\mc X, d)$ in some metric space $M$. Let $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ be a clustering of $\mc X$ induced by centers $\mu_1, \ldots, \mu_k \in M$ which satisfies $\gamma$-margin and $\mu_i$ is the mean of points in $C_i$. 
Assume $\delta \in (0, 1)$ and $\gamma > 1$. Then with probability at least $1-\delta$, Alg. \ref{alg:steinerQueryPositive} outputs $C^*$.
\end{theorem}

\begin{proof}
In the first phase of the algorithm we are making $l>k\eta$ queries. Therefore, using the pigeonhole principle, we know that there exists cluster index $p$ such that $|Z_p| > \eta$. Then Lemma \ref{lemma:phase1} implies that the algorithm chooses a center $\mu_p'$ such that with probability at least $1-\frac{\delta}{k}$ we have $d(\mu_p, \mu_p') \le r(C_p)\epsilon$. By Lemma \ref{lemma:hasGammaMargin}, this would mean that $d(x, \mu_p') < d(y, \mu_p')$ for all $x \in C_p$ and $y \not\in C_p$. Hence, the radius $r_i$ found in the phase two of Alg. \ref{alg:steinerQueryPositive} is such that $r_{i} = \max\limits_{x \in C_p} d(x, \mu_p')$. This implies that $C_p'$ (found in phase two) equals to $C_p$. Hence, with probability at least $1-\frac{\delta}{k}$ one iteration of the algorithm successfully finds all the points in a cluster $C_p$. Using union bound, we get that with probability at least $1-k\frac{\delta}{k} = 1-\delta$, the algorithm recovers the target clustering.
\qed 
\end{proof}

\begin{theorem}
\label{thm:steinerQueryPositiveComplexity}
Let the framework be as in Thm. \ref{thm:steinerQueryPositive}. Then Alg. \ref{alg:steinerQueryPositive} 
\begin{itemize}[nolistsep,noitemsep]
\item Makes $O\big(k^2\log |\mc X| + k^3\frac{\log k + \log (1/\delta)}{(\gamma - 1)^4}\big)$ equivalence queries to the oracle $\mc O$.
\item Runs in $O\big(k|\mc X|\log |\mc X| + k^2\frac{\log k + \log (1/\delta)}{(\gamma - 1)^4}\big)$ time.
\end{itemize}
\end{theorem}

\begin{proof}
The first phase of the algorithm takes $O(k\eta)$ time and makes $k\eta+1$ membership queries to the oracle. The second phase takes $O(n\log n)$ times and makes $O(\log n)$ membership queries to the oracle. Hence, in total the algorithm takes $O(kn\log n + k^2\eta)$ time and makes $O(k\log n + k^2\eta)$ membership queries. Substituting $\eta = c\frac{\log k + \log(1/\delta)}{\epsilon^4}$ and using the fact that each membership query can be substituted by $k$ equivalence queries (see appendix \ref{appendix:diffQueryModels}) finishes the proof.
\qed
\end{proof}

\section{Lower Bounds}
\label{section:lowerBounds}
In section \ref{section:clusteringWithQuery}, we proposed an algorithm which for $\gamma > 1$, makes $O(k\log n)$ queries and in polynomial time finds the target clustering with very high probability. In this section, we want to give a lower bound for our problem setting. We will show that if $\gamma < 1.4$ then finding the optimal euclidean $k$-means clustering is NP-Hard without queries. Note that Thms. \ref{thm:steinerQueryPositive} and \ref{thm:steinerQueryPositiveComplexity} shows that we can overcome this lower bound using queries. Next, we also give a lower bound on the number of queries needed. We show that making `too few' (sublogarithmic in $k$) queries also doesn't help and the problem remains NP-Hard.

\subsection{Lower bounds without query}
Given a clustering instance $(\mc X, d)$ in euclidean space and the number of clusters $k$. The $k$-means tries to find a clustering $\mc C = \{C_1, \ldots, C_k\}$ which minimizes the following objective function. $\sum\limits_{C_i}\frac{1}{|C_i|} \sum_{x, y \in C_i} d^2(x, y)$. \cite{vattani2009hardness} considered the weighted version of this problem where every point $x$ has a weight $w(x)$. $$\sum\limits_{C_i}\frac{1}{\sum_{x \in C_i}w(x)} \sum_{x, y \in {C_i \choose 2}} w(x)w(y)d^2(x, y).$$ Vattani showed that finding the optimal solution to the weighted $k$-means objective is NP-Hard even in Euclidean plane. We will use the same reduction technique and ideas as Vattani's proof to get our lower bound. However to prove the lower bound under gamma margin, we need to modify the author's original proof and argue more carefully.

\begin{theorem}
For any $\gamma < 1.4$, finding the optimal solution to the $k$-means objective function is NP-Hard even when the optimal satisfies $\gamma$-margin.
\end{theorem}
To prove the theorem, we will reduce an instance of exact cover by 3-sets (X3C) to the decision version of weighted $k$-means (which asks for a clustering with cost $\le L$).
\begin{definition}[X3C]
Given a set $U$ containing exactly $3n$ elements and a collection $\mc S = \{S_1, \ldots, S_l\}$ of subsets of $X$ such that each $S_i$ contains exactly three elements. Does there exist $n$ elements in $\mc S$ such that their union is $U$? 
\end{definition}

\subsubsection{Component design}
We will now describe the reduction from X3C to the $k$-means problem. We use the same reduction that was used by \cite{vattani2009hardness}. 

The component $H_{l,n}$ and its parameters are described in Fig. \ref{fig:lowerBoundComponent}. The row $R_i$ is composed of $6n + 3$ points $\{s_i, r_{i, 1}, \ldots, r_{i, 6n+1}, f_i\}$. The points $r_{i, j}$ have weight $w$ and points $s_i$ and $f_i$ have weight $2w$. Row $M_i$ is composed of $3n$ points $\{m_{i,1}, \ldots, m_{i, 3n}\}$ of weight $2w$. The distances between the points are shown in Fig. \ref{fig:lowerBoundComponent}. We choose $k = (l-1)3n + l(3n+2)$. 

We choose the following values of the parameters. $h = \sqrt{2}, d = \sqrt{4.5}, \epsilon = \frac{1}{w^2}$ and $\alpha = \frac{d}{w}-\frac{1}{2w^3}.$

\begin{definition}[$A$ and $B$ clustering of $R_i$ \cite{vattani2009hardness}]

\noindent $A$ - For $1 \le j \le 3n$, $R_i$ has clusters $\{r_{2j-1}, r_{2j}\}$. Also, it has clusters $\{s_i\}, \{r_{i, 6n+1}, f_i\}$\\
\noindent $B$ - For $1 \le j \le 3n$, $R_i$ has clusters $\{r_{2j}, r_{2j+1}\}$. Also, it has clusters $\{s_i, r_{i, 1}\}, \{f_i\}$
\end{definition}
A row grouped in a $A$ clustering costs $(6n+3)w-\alpha$ while a row in $B$ clustering costs $(6n+3)w$. Hence, the clustering with the rows $R_i$ grouped in a $A$ or $B$ clustering and the rows $M_i$ in single clusters is a $k$-clustering and costs atmost $L_1 := (6n+3)lw$. We will call such a clustering `{\it nice}'.

\begin{lemma}[Similar to Lemma 7 in \cite{vattani2009hardness}]
Any non-nice clustering of $H_{l, n}$ costs atleast $> L_1 + \frac{7}{8}w$ where $w = poly(l, n)$
\end{lemma}

\begin{proof}
Any non-nice clustering $C$ can be of the following different forms.
\begin{itemize}[nolistsep]
\item Contains $m_{i, j}, m_{i,j+1}$ in the same cluster - $C$ must have either of the following. (1) $r_{i, j}$ and $r_{i,j+1}$ as a singleton. In this case, the new cost is atleast $L_1 + 8w - 2w = L_1+6w$. (2) $s_i$ and $r_{i,1}$ as singleton. In this case, the difference of cost is atleast $8w - 3w = 5w$. (3) $f_i$ and $r_{i,6n+1}$ as singleton. In this case, the difference of cost is atleast $8w - (3w-\alpha) = 5w+\alpha$. Note that having multiple $m_{i, j}$'s in the same cluster or having multiple clusters which contain $m_{i, j}, m_{i,j+1}$ will bring the cost up even furthur.
\item Contains $r_{i, 2j}, m_{i, j}$ in the same cluster - $C$ must have either of the following. (1) $r_{i, j'}$ as a singleton. In this case, the cost difference is atleast $ \frac{2w}{3}(\sqrt2+1)^2 - 2w > \frac{15}{8}w$. (2) $s_i$ and $r_{i,1}$ as singleton. In this case, the difference of cost is atleast $\frac{2w}{3}(\sqrt2+1)^2 - 3w > \frac{7}{8}w$. (3) $f_i$ and $r_{i,6n+1}$ as singleton. In this case, the difference of cost is atleast $\frac{2w}{3}(\sqrt2+1)^2 - (3w-\alpha) > \frac{7}{8}w+\alpha$. Note that having multiple clusters which contain $m_{i, j}, r_{i',j'}$ will bring the cost up even furthur.
\item Contains $r_{i,j}$ as singleton - $C$ must contain both $\{s_i, r_{i,1}\}$ and $\{r_{i,6n+1}, f_i\}$. Hence the increase in cost is atleast $3w - 2w = w$.
\item Contains multiple $\{r_{i,j}\}$ in the same cluster - Assume the cluster has cardinality $m > 2$. Using simple counting argument (as has also been shown in  \cite{vattani2009hardness}), this cluster costs atleast $\frac{w}{3}m(m^2-1)$. A nice clustering would cost atmost $w(m + \lceil{\frac{m}{2}}\rceil - 2) + 3w = w(m + \lceil{\frac{m}{2}}\rceil + 1)$. Hence, the difference in cost is atleast $2w$.
\end{itemize}
\qed
\end{proof}

\subsubsection{Reduction and proof}
Given an instance of X3C, that is, given elements $U = \{1, \ldots, 3n\}$ and collection $\mc S$, we construct an instance $X$ of $k$-means as follows. $X = H_{l,n} \cup (\cup_{i=1}^l Z_i)$ where $Z_i$ depends on the set $S_i$. 

The set $Z_i$ is shown in Fig. \ref{fig:setZDescription}. The construction is same as \cite{vattani2009hardness}. For every $j\in U$, there are four possible locations $x_{i, j}, x_{i,j'}, y_{i,j}$ and $y_{i, j}'$. One of $x_{i,j}$ and $x_{i,j}'$ will be occupied and one of $y_{i,j}$ and $y_{i,j}'$ will be occupied. Each point has weight $\frac{w}{2}$.
\begin{itemize}[nolistsep,noitemsep]
\item $j \in S_i \iff x_{i,j}' \in Z_i$
\item $j \not\in S_i \iff x_{i,j} \in Z_i$
\item $j \in S_{i+1} \iff y_{i,j}' \in Z_i$
\item $j \not\in S_{i+1} \iff y_{i,j} \in Z_i$
\end{itemize}

Hence, our clustering instance $X = H_{l,n} \cup Z$. The number of clusters $k = (l-1)3n + l(3n+2)$ and the cost $L = L_1 + L_2 -n\alpha$. We choose $L_2 = 6n(l-1)\frac{2w\frac{w}{2}}{2w+\frac{w}{2}}h^2 = 6n(l-1)$.


Use a variant of Vattani proof.

\subsection{Lower bound with sub-logarithmic queries}
Proof as simulation.


\section{Discussion and conclusion}
Other related works and conclusion.

\bibliographystyle{alpha}
\bibliography{activeClustering}

\appendix
\section{Relationship Query Models}
\label{appendix:diffQueryModels}

\begin{proposition}
Any clustering algorithm that uses only $q$ equivalence queries can be adjusted to use $2q$ membership queries (and no equivalence queries) with the same order of time complexity.
\end{proposition}
\begin{proof}
We can replace each equivalence query with two membership queries as in $Q(x_1,x_2)={\mathbbm{1}}\{Q(x_1)=Q(x_2))\}$.
\end{proof}

\begin{proposition}
Any algorithm that uses only $q$ membership queries can be adjusted to use $kq$ equivalence queries (and no membership queries) with at most a factor $k$ increase in computational complexity, where $k$ is the number of clusters.
\end{proposition}
\begin{proof}
If the clustering algorithm has access to an instance from each of $k$ clusters (say $x_i\in X_i$), then it can simply simulate the membership query by making $k$ equivalence queries ($Q(x) = \argmax_{i}\mathbbm{1}\{Q(x, x_i)\}$). Otherwise, assume that at the time of querying $Q(x)$ it has only instances from $k\prime<k$ clusters. In this case, the algorithm can do the same with the $k\prime$ instances and if it does not find the cluster, assign $x$ to a new cluster index. This will work, because in the clustering task the output of the algorithm is a partition of the elements, and therefore the indices of the clusters do not matter.
\end{proof}


\section{Comparison of $\gamma$-margin and $\alpha$-center proximity}
\label{appendix:gammaMrginVsAlphaCenter}



\section{Center Proximity}
We are given a clustering instance $(\mc X, d)$ in some metric space $M$. A center-based clustering is induced by centers $c_1, \ldots, c_k \in M$ where each point $x \in \mc X$ is assigned to its closest center.

\begin{definition}[$\alpha$-center proximity]
\label{defn:alphacp}
Given a clustering instance $(\mc X, d)$ and the number of clusters $k$. We say that a clustering $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ induced by centers $c_1, \ldots, c_k \in M$ has $\alpha$-center proximity w.r.t $\mc X$ and $k$ if the following holds. For all $x \in C_i$ and $i\neq j$, 
$$\alpha d(x, c_i) < d(x, c_j)$$
\end{definition}

\subsection{Goal}
We are given a dataset $\mc X$. $\mc X$ has a target clustering $\mc C_{\mc X}$ which satisfies $\alpha$-center proximity. Our goal is to recover the target clustering $\mc C_{\mc X}$. There is a catch. Our algorithm has access to an {\it oracle} which can provide answers to membership queries. That is, we can pose the following question to the oracle.
\begin{center}
  \begin{tabular}{l}
	{\it Question:} Does $x \in \mc X$ belong to the $i^{th}$ cluster $C_i$ ? \\
	{\it Oracle:} Answers `yes' or `no'
  \end{tabular}
\end{center}

\noindent Our goal is to design an algorithm which given a set $\mc X$ and an oracle $\mc O$ outputs the target clustering $\mc C_{\mc X}$ while making as few queries to the oracle as possible. Note that it is always possible recover the target clustering by making $n = |\mc X|$ queries to the oracle. Hence, we will require that any algorithm make sub-linear queries to the oracle. 

\subsubsection*{Problem Setting}
\begin{itemize}[nolistsep, noitemsep]
\item {\it Steiner} - In this setting, centers $c_1, \ldots, c_k$ can be arbitrary points in the metric space.
\item {\it Restricted} - The centers are part of the data-set $\mc X$, that is, $c_1, \ldots, c_k \in \mc X$ \\
\end{itemize}
We will give algorithms in both the settings. Note that restricted setting is similar to working with $k$-median cost function (if we consider the target clustering to be the optimal $k$-median clustering). Similarly, the optimal $k$-means solution can be thought of as a target in the steiner setting.

\subsection{Related work}
\label{section:relatedwork}
For $\alpha$-center proximity, the following results are known when no oracle (or queries) is available to the clustering algorithm. In the restricted setting, the algorithm of Balcan and Liang \cite{balcan2012clustering} `recovers' the target clustering (outputs a tree such that the target is a pruning of the tree) when $\alpha > \sqrt{2} + 1$. In this work, we will show that in the presence of queries, we can recover the target clustering as long as $\alpha > 2$. Note that this is still above the NP-Hardness lower bound of $\alpha < 2$ in the restricted setting (\cite{ben2014data}).

In the steiner setting, the algorithm of Awasthi et. al \cite{awasthi2012center} recovers the target clustering when $\alpha > 2+\sqrt{3}$. We show that in the presence of few queries, we can recover the target clustering if $\alpha > \sqrt{2}+1$. This is much better than the previous known result in the absence of queries. This result also beats the NP-Hardness lower bound of $\alpha < 3$ in the steiner setting (\cite{awasthi2012center}).

\subsection{Algorithm}


\subsection{Lower bound}
In the steiner setting, Awasthi et. al \cite{awasthi2012center} showerd that for $\alpha < 3$ it is NP-Hard to find the optimal solution to the $k$-median objective function. Our positive result (Thm. \ref{thm:steinerQueryPositive}) is for the $k$-means optimal clustering. Hence, we would like to prove lower bounds in this setting. We will show that for $\alpha < 3$, it is NP-Hard to approximate the $k$-means cost function as well. The proof uses a simple reduction from the $k$-median instance to the $k$-means instance and is stated below.

\begin{theorem}
For any $\alpha < 3$, finding the optimal solution to the $k$-means objective function is even when the optimal solution satisfies $\alpha$-center proximity is NP-hard.
\end{theorem}

\begin{proof}
We will first state both the $k$-median and the $k$-means optimization problem under $\alpha$-center proximity and then show the reduction.

\vspace{1em}\noindent $k$-media clustering\\
{\it Input:} A set $\mc X$, integer $k$ and metric $d$. It is known that the desired solution satisfies $\alpha$-center proximity.\\
{\it Output:} A partition of $\mc X \subseteq M$ into $k$ clusters $C_1, \ldots, C_k$ with a center $c_i \in M$ for each cluster so as to minimize $\sum_{C_i}\sum_{x \in C_i} d(x, c_i)$.  

\vspace{0.5em}\noindent $k$-means clustering\\
{\it Input:} A set $\mc X$, integer $k$ and metric $d$. It is known that the desired solution satisfies $\alpha$-center proximity.\\
{\it Output:} A partition of $\mc X \subseteq M$ into $k$ clusters $C_1, \ldots, C_k$ with a center $c_i \in M$ for each cluster so as to minimize $\sum_{C_i}\sum_{x \in C_i} d^2(x, c_i)$.  

\vspace{1em}\noindent Awasthi et. al \cite{awasthi2012center} showed that the $k$-median problem above is NP-Hard. Given an instance of $k$-median problem we construct an instance of $k$-means as follows. We, let $\mc X, k$ be the same. But we use a new metric $\tilde d = \frac{d}{\sqrt{|\mc X|}}$. We will show that if the $k$-median instance has cost $\le A$ if and only if $k$-means instance has cost $\le A^2$. In the proof, we use $c(x)$ to denote the center of $x$. This will prove our desired result.

\noindent$\Rightarrow$ For the forward direction, observe that $\sum_x \tilde d^{2}(x, c_i) \le\sum_{x}d^2(x, c(x)) \le (\sum_{x} d(x, c_i))^2 \le A^2$.

\noindent$\Leftarrow$ For the reverse direction, observe that $(\sum_{x}d(x, c(x)))^2 \le n\sum_{x} d^2(x, c_i) = \sum_x \tilde d^{2}(x, c_i) \le A^2$.
\end{proof}








\section{Concentration inequalities}
\label{appendixsection:conIneq}

\begin{theorem}[Generalized Hoeffding's Inequality (e.g., \cite{ashtiani2015dimension})]
\label{thm:genHoeff}
Let $X_1, \ldots. X_n$ be i.i.d random vectors in some Hilbert space such that for all $i$, $\|X_i\|_2 \le R$ and $E[X_i] = \mu$. If $n > c\frac{\log(1/\delta)}{\epsilon^2}$, then with probability atleast $1-\delta$, we have that
$$\Big\|\mu - \frac{1}{n}\sum X_i\Big\|_2^2 \le R^2\epsilon$$ 
\end{theorem}

\section{Some properties of Center Proximity (maybe useful later)}
\begin{lemma}
\label{lemma:hasPropertyR}
Given a clustering instance $(\mc X, d)$ in some metric space $M$. Let $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ be a clustering of $\mc X$ induced by centers $c_1, \ldots, c_k \in M$ which satisfies $\alpha$-center proximity. Let $c_i' \in M$ such that $d(c_i, c_i') \le r(C_i)\epsilon$. If $\alpha \ge 2 + 3\epsilon$ then for all $x \in C_i$ and for all $y \in C_j$
$$d(x, c_i') < d(x, y)$$  
\end{lemma}

\begin{proof}
Fix any $x \in C_i$ and $y \in C_j$. Let $c_i, c_i'$ and $c_j$ be as defined in the statement of the lemma. Let $x^*$ be such that $d(x^*, c_i) = r(C_i)$. The proof of our lemma will follow from the following two properties of $\alpha$-center proximity instances.
\begin{enumerate}[nolistsep,noitemsep]
\item $r(C_i) < \frac{1}{\alpha-1}d(c_i, c_j)$.
\begin{flalign*}
&(\alpha-1) r(C_i) = \alpha d(x^*, c_i) - d(x^*, c_i) < d(x^*, c_j) - d(x^*, c_i) \le d(c_i, c_j)&
\end{flalign*}
\item $d(c_i, c_j) < \frac{\alpha+1}{\alpha-1}d(x, y)$
\begin{flalign*}
&\text{Using the triangle inequality, we get that}&\\
&d(c_i, c_j) \le d(x, y) + d(x, c_i) + d(y, c_j). \text{From Corollary $2.3$ in \cite{awasthi2012center}, we know that}&\\
&d(x, c_i), d(y, c_j)< d(x, y)/(\alpha-1) \text{ which completes the proof of the result.}&
\end{flalign*}
\end{enumerate}
\begin{flalign*}
&\text{Now to prove the lemma, observe that using triangle inequality, we get that
}&\\
&d(x, c_i') \le d(c_i, c_i') + d(x, c_i) < \frac{d(x, y)}{(\alpha-1)} + \epsilon r(C_i) < \bigg[ \frac{1}{\alpha-1} + \frac{\epsilon(\alpha+1)}{(\alpha-1)^2}\bigg]d(x, y) = \frac{(1+\epsilon)\alpha-1+\epsilon}{(\alpha-1)^2}d(x, y)&
\end{flalign*}
Observe that, $(\alpha-1)^2 \ge (1+\epsilon)\alpha -1 + \epsilon \iff \alpha^2 -(3+\epsilon)\alpha + 2-\epsilon \ge 0$
$\iff \alpha \ge \frac{3+\epsilon + \sqrt{1+10\epsilon + \epsilon^2}}{2}$ or $\alpha \le \frac{3+\epsilon - \sqrt{1+10\epsilon + \epsilon^2}}{2}$. Now, for $\alpha \ge 2 + 3\epsilon \implies \alpha \ge \frac{3+\epsilon + \sqrt{(1+5\epsilon)^2}}{2}$ which completes the proof of the lemma.
\end{proof}

\end{document}


