\documentclass[orivec]{llncs}
\usepackage{llncsdoc}

\def\COMPLETE{}
\usepackage[boxruled]{algorithm2e}
\usepackage{amsmath,amssymb,amstext}
\usepackage[margin=1in]{geometry}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[toc,page]{appendix}

\usepackage{color}
\usepackage[toc,page]{appendix}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{times}
\usepackage{float}
\usepackage{capt-of}

\usepackage{bbm}

\usepackage{tikz}
\usetikzlibrary{shapes, calc, arrows, through, intersections, decorations.pathreplacing, patterns}

\newcommand{\mc}{\mathcal}
\newcommand{\mb}{\mathbf}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\vcdim}{VC-Dim}
\DeclareMathOperator{\vol}{vol}

\renewcommand{\qed}{\hfill\ensuremath{\blacksquare}}
\renewcommand\labelitemi{$\bullet$}
\renewcommand{\labelitemii}{$\star$}

\makeatletter  %% this is crucial
\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}%
   {-18\p@ \@plus -4\p@ \@minus -4\p@}%
   {8\p@ \@plus 4\p@ \@minus 4\p@}%     <-- this
   {\normalfont\normalsize\bfseries\boldmath
   \rightskip=\z@ \@plus 8em \pretolerance=10000}}
\makeatother   %% this is crucial
\setcounter{secnumdepth}{3}


\newcommand{\todo}{\textcolor{blue}{[TODO]}\xspace}
\newcommand{\complete}{\textcolor{red}{[TO BE COMPLETED]}\xspace}
\newcommand{\wip}{\textcolor{red}{[Work in progress]}\xspace}
\newcommand{\multlinecomment}[1]{\directlua{-- #1}}




\newcommand{\fix}{\textcolor{blue}{[FIX]}\xspace}
\newcommand{\q}{\textcolor{blue}{[?]}\xspace}

\title{Clustering with Oracles}
\author{Student submission}
%\institute{School of Computer Science\\University of Waterloo\\ Waterloo, ON, N2L 3G1 \\CANADA \\ \email{\{skushagr@,shai@cs.\}uwaterloo.ca}}


%%%%% Document Body %%%%%
\begin{document}
\maketitle

\begin{abstract}
We provide a framework for clustering where the learner is allowed to ask \emph{same-cluster} queries from an oracle (e.g., a domain expert). In this type of query, the oracle indicates whether two given instances belong to the same cluster or not. We study the query and computaitonal complexity of clustering in this framework. In order to do that, we consider a setting where the oracle conforms to a center-based clustering with a notion of margin.  

r

\end{abstract}

\section{Introduction}

Clustering is a challenging task particularly because of the following two reasons. The first problem is that the clustering problem for each task is usually \emph{under-specified}. The second one is that performing clustering under many natural models is computationally hard.

Consider the task of dividing the users of an online shopping service into different groups. The result of this clustering can then be useful for example in suggesting similar products to the users in the same group, or just in organizing data so that it would be easier to read/analyze the monthly purchase reports. In this case, it is clear that one needs to exploit domain knowledge to avoid an ill-defined problem, as there are many different choices for clustering that can give different results. 

Aside from trial and error, a principled way of extracting domain knowledge is to perform clustering using a form of `weak' supervision.  For example, Balcan and Blum \cite{balcan2008clustering} propose to use an interactive framework with 'split/merge' queries for clustering. In another work, Ashtiani and Ben-David \cite{ashtiani2015representation} require the domain expert to provide the clustering of a 'small' subset of data.

At the same time, mitigating the computational problem of clustering is critical. Solving most of the common optimization formulations of clustering is NP hard (in particular, solving the well known $k$-means and $k$-median clustering problems). One approach to address this issues is to exploit the fact that natural data sets usually exhibit some 'nice properties' that enable us to avoid the worst-case scenario and find the optimal solution to clustering efficiently. The quest for searching such notions of niceness is still ongoing, and it should be noted that the niceness properties that are currently analyzed are too strict to happen in real world situations (see \cite{Ben-David15} for a critical survey of work in this direction.

In this work, we take a new approach to alleviating the computational problem of clustering. In particular, we ask the following question: can weak supervision (in a natural form of queries) help relaxing the computational burden of clustering? This will add up to the other benefit of supervision: making the clustering problem well-defined by injecting the domain knowledge through supervised feedback.

The general setting considered in this work is the following. Let $X$ be a set of elements that should be clustered. The oracle (e.g., a doamin expert) has some information about a  target clustering $C^*_X$ in mind. The clustering algorithm has access to $X$, and can make queries in order to recover $C^*_X$. The queries are in the form of \emph{same-cluster} queries. This means that the algorithm can ask whether two elements belong to the same cluster or not.

We will also consider the case that the oracle has the optimal $k$-means solution in his mind. We then try to find out whether access to a 'reasonable' number of same-cluster queries can enable us to provide an efficient algorithm for an otherwise NP-hard problem. 


\subsection{Contributions}
We introduce the notions of clustering with same-cluster/cluster-assignment queries. We further define a new notion of niceness of data called $\gamma$-margin property and contrast it to the previously studied notions, showing that this is a cenral notion that has not been studied explicitly. We further show that $\gamma$-margin assumption is not necessarily strong enough to make the problem computationally easy. In particular, we show that $k$-means clustering under $\gamma$-margin assumption is NP-hard for $\gamma < 1.5$.

In the next step, we provide a probabilistic polynomial time (BPP) algorithm for clustering with queries, under the assumption that $\gamma > 1$. More specifically, this algorithm makes $O\big(k^2\log k\log n)$ same-cluster queries to the oracle and runs in $O\big(kn\log n + k^2\log k)$ time, where $k$ is the number of clusters and $n$ is the size of the sample set.

These results when contrasted to each other show an interesting phenomenon. Assume that the oracle has the optimal $k$-means solution in his mind and it satisfies $\gamma$-margin property for $1<\gamma <1.5$. In this case our provided lower bound means that $k$-means clustering is NP-hard without making queries, while the positive result shows that with making a reasonable number of queries the problem becomes efficiently solvable. This indicates a nice trade off between query complexity and computational complexity. We further show a lower bound on the number of queries needed to make the problem efficiently solvable, showing that with access to only $O(\log k + \log n)$ queries the problem remain NP-hard.

\subsection{Related Work}

The use of supervision in clustering is not unprecedented. Balcan et. al \cite{balcan2008clustering} provided a framework for interactive clustering with the help of a user (i.e., an oracle). The form of queries considered in this framework are different with ours. In particular, the oracle is provided with the current clustering, and tells the the algorithm to either split a cluster or merge two clusters. Note that in this setting, the oracle should be able to evaluate the whole given clustering for each query.

Another example of the use of supervision in clustering was provided by Ashtiani and Ben-David \cite{ashtiani2015representation}. They assumed that the target clustering can be approximated by first mapping the data points to a new space and then performing $k$-means clustering. The supervised data (i.e., the clustering of a small subset of data) was then used to find such mapping.

The computational complexity of clustering has been extensively studied. Many of these results are negative, showing that clustering is computationally hard. For example, $k$-means clustering is NP-hard even for $k=2$ \cite{dasgupta2008hardness}, or in a 2-dimensional plane \cite{vattani2009hardness,mahajan2009planar}. In order to alleviate the problem of computational complexity, some notions of niceness of data under which the clustering becomes easy have been considered.  

Awasthi et. al \cite{awasthi2012center} introduced the notion of $\alpha$-center proximity. In the case when the centers of the clusters must come from the data set (e.g. the optimal $k$-median solution), their algorithm `recovers' the target clustering (outputs a tree such that the target is a pruning of the tree) for $\alpha > 3$.  Balcan and Liang \cite{balcan2012clustering} improve the constant to $\alpha > \sqrt{2} + 1$. Ben-David and Reyzin \cite{ben2014data} show that this problem is NP-Hard for $\alpha < 2$. For the other case when the centers are unrestricted (e.g. the optimal $k$-means clustering), the algorithm of Awasthi et. al \cite{awasthi2012center} recovers the target clustering when $\alpha > 2+\sqrt{3}$. They also show that the problem is NP-Hard for $\alpha < 3$.

\section{Problem Formulation}



\subsection{Clustering with Query}

Let $\mc X$ be domain set and $d:(\mc X, \mc X) \rightarrow \mc R $ be a distance function over its elements. Let $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ be a clustering (i.e., a partitioning) of $\mc X$. We say $x_1 \overset{C_{\mc X}}{\sim} x_2$ if $x_1$ and $x_2$ belong to the same cluster according to $C_{\mc X}$. We further denote by $n$ the number of instances ($|{\mc X}|$) and by $k$ the number of clusters.

We assume that an oracle $\mc O$ has a clustering $C^*_{\mc X}=\{ C^*_1, \ldots C^*_k\}$ in his mind. The clustering algorithm then tries to recover $C^*_{\mc X}$ by querying the oracle. The following notions of query are natural for clustering tasks.

\begin{definition}[Same-cluster Query]
A same-cluster query asks whether two instances $x_1$ and $x_2$ belong to the same cluster, i.e., 
$${\mc O}(x_1, x_2) = \left\{
	\begin{array}{ll}
		\mbox{true }  & \mbox{if } x_1 \overset{C^*_{\mc X}}{\sim} x_2   \\
		\mbox{false } & o.w. 
	\end{array}
\right. $$
\end{definition}

\begin{definition}[Cluster-assignment Query]
A cluster-assignment query asks the cluster index that an instance $x$ belongs to. In other words ${\mc O}(x) = i$ if and only if $x \in C^*_i$.
%$$Q(x, i) = \left\{
%	\begin{array}{ll}
%		\mbox{true }  & \mbox{if } x \in C^*_i   \\
%		\mbox{false } & o.w. 
%	\end{array}
%\right. $$
\end{definition}

Same-cluster queries seem to be more natural for clustering tasks, as the oracle (or the domain expert) will only need to respond to binary equivalence questions. In appendix \ref{appendix:diffQueryModels} we show that these two notions of query are related, in the sense that they give relatively the same power to the clustering algorithm. This is useful for some of our proofs, because using cluster-assignment queries makes the analysis more transparent.

An active clustering instance is determined by the tuple $(\mc X, d, C^*)$. A clustering algorithm $\mc A$ is called a $q$-solver of this instance if it can recover $C^*$ by having access to $(\mc X, d)$ and making at most $q$ queries. This algorithm is further called a polynomial $q$-solver if its time-complexity is polynomial in $|\mc X|$ and $|C^*|$.

\begin{definition}[Query Complexity]
\label{definition:QueryComplexity}
Let $G$ be a set of clustering instances. We say $G$ admits an $O(q)$ query complexity if there exists an algorithm $\mc A$ that is a polynomial $q$-solver for every clustering instance in $G$.
\end{definition}

\subsection{Center-based Clustering with Queries}

The framework inroduced in the previous section is general and can be used in different settings. In this section, we make more assumptions about the problem setting in order to make it concrete.

We say that a clustering $C_{\mc X}$ is \emph{center-based} if there exists a set of corresponding centers $\mc \mu = \{\mu_1, \ldots, \mu_k\}$ such that the elements of each cluster are closer to their center than any other center. More formally, for every element $x$ in $\mc X$ we should have $x\in C_i \Leftrightarrow i=\argmin_j d(x,\mu_j)$. A clustering instance $(\mc X, d, C^*)$ is center-based if $C^*$ is a center-based clustering of $\mc X$.

Some of the most-used clustering methods (e.g., k-means and k-median clustering) are center-based. However, clustering is genrally computationally hard in these settings. Therefore, some notions of \emph{niceness} of the clustering instance (under which clustering becomes \emph{easy}) have been considered in literature. In the following we introduce a related notion of niceness.

\begin{definition}[$\gamma$-Margin Property]
\label{defn:alphacp}
We say that a center-based clustering instance $(\mc X, d, C^*)$ has $\gamma$-margin property if for every $C_i$ in $C^*$ the following holds

$$\forall x\in C^*_i, y \notin C^*_i, \gamma d(x, \mu_i) < d(y, \mu_i)$$

where $\mu_i$ is the center corresponding to $C_i$.

\end{definition}


%, a center-based clutering algorithm outputs a set of centers $\mu$ (or the corresponding partitioning $C_{\mc X}$ based on a criterion. This criterion can be the optimization of a certain objective fucntion (e.g., k-means objective function). In the next section, we introduce a different setting where the algorithm tries to find out the clustering that an oracle has in mind by querying the oracle. 

%\begin{definition}[$\gamma$-margin]
%\label{defn:alphacp}
%Given a clustering instance $(\mc X, d)$ from some metric space $M$ and the number of clusters $k$. We say that a clustering $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ induced by centers $\mu_1, \ldots, \mu_k \in M$ has $\gamma$-margin w.r.t $\mc X$ and $k$ if the following holds. For all $x \in C_i$ and $y \in C_j$, 
%$$\gamma d(x, \mu_i) < d(y, \mu_i)$$
%\end{definition}

In clustering literature, similar notions have been considered before. One of these notions is $\alpha$-center proximity \cite{balcan2012clustering,awasthi2012center}. Comparison of our notion of margin against center proximity is orthogonal to the problem being addressed in this paper, and is provided in appendix \ref{appendix:gammaMrginVsAlphaCenter}. We just mention here that the results (both upper and lower bounds) obtained under $\alpha$-center proximity can also be obtained under $\gamma$-margin for `reasonably small' values of $\gamma$.
 
Throughout the next sections, we assume that the instaces are in a Euclidean metric space (i.e., $\mc X\subset \mathbb{R}^d$). Furthermore, we assume that the clustering instance $(\mc X, d, C^*)$ is center-based and admits a $\gamma$-margin property\footnote{Naturally, the larger the value of $\gamma$, the stronger this assumtion is.}. Finally, we assume that the centers $\mu^*$ corresponding to $C^*$ are the centers of mass of the corresponding clusters. In other words, $\mu^*_i=\frac{1}{|C_i|}\sum_{x\in C^*_i} x$. Note that this is the case for example when the oracle's clustering is the optimal solution to the Euclidean k-means clustering problem.

\section{An Algorithm for Clustering with Query}
\label{section:clusteringWithQuery}

In this section we provide an efficient algorithm for clustering with queries. The setting is the one described in the previous section. In particular, it is assumed that the oracle has a $\gamma$-margin center-based clustering in his mind. The space is Euclidean and the center of each cluster is the center of mass of the instances in that cluster. The algorithm makes cluster-assignment queires to the oracle, but this can be translated to binary same-cluster queries (see appendix \ref{appendix:diffQueryModels}). 

%We will assume that the target clustering $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ induced by centers $\mu_1, \ldots, \mu_k$ has the following property. For all $i$, $\mu_i$ is the mean of all the points in cluster $C_i$, that is,
%\begin{align}
%\mu_i = \frac{1}{|C_i|}\sum_{x \in C_i} x \stepcounter{equation}\tag{P\theequation} \label{property:targetClust}
%\end{align}
%Note that the optimal solution to the euclidean $k$-means cost has this property. Our algorithm is more general and works for any target clustering which has the above property (\ref{property:targetClust}).

Intuitively, our algorithm (Algorithm  \ref{alg:steinerQueryPositive}) does the following. In the first phase, it tries to approximate the center of on of the clusters. It does this by querying points from $\mc X$ uniformly at random until it has a sufficient number of points from at least one cluster (say $C_p$). It uses the mean of these points, $\mu_p^\prime$, to approximate the cluster center. 

In the second phase, the algorithm recovers all of the instances belonging to $C_p$. In order to do that, it first sorts all of the instances based on their distance to $\mu_p^\prime$. By showing that all of the points in $C_p$ lie inside a sphere centered at $\mu_p^\prime$ (which does not include points from any other cluster), it tries to find the radius of this sphere by doing binary search using cluster-assignment queries. After that, the elements in $C_p$ will be located and can be removed from the data set. The algorithm repeats this process $k$ times to recover all of the clusters.

%After finding the point in $C_p$ with the maximum distance to $\mu_p\prime$ (we call this point $b_{idx}$), it puts all of the points in the sphere

%$\mu_p'$. Then, it clusters and deletes all points whose distance is $\le d(b_{idx}, \mu_p')$. We will show that this process guarantees that (as long as $\mu_p'$ is a good approximation of the actual center $\mu_p$) and $\gamma > 1$, all the points from $C_p$ are now recovered. We then repeat this process $k$ times to recover all the clusters.

The details of our approach is stated precisely in Algorithm \ref{alg:steinerQueryPositive}. Note that $\beta$ is a small constant\footnote{It corresponds to the constant appeared in generalized Hoefding inequality bound, discussed in Theorem \ref{thm:genHoeff}}. Theorem \ref{thm:steinerQueryPositive} shows that if $\gamma > 1$ then our algorithm recovers the target clustering with high probability. Next, we give bounds on the time and query complexity of our algorithm. Theorem \ref{thm:steinerQueryPositiveComplexity} shows that our approach needs $O(k\log n)$ queries and runs with time complexity $O(kn\log n)$.

\RestyleAlgo{boxruled} 
\SetAlgoNoLine
\begin{algorithm}[h]
 \KwIn{Clustering instance $\mc X$, oracle $\mc O$, the number of clusters $k$ and parameter $\delta \in (0, 1)$}
 \KwOut{A clustering $\mc C$ of the set $\mc X$}

 \vspace{0.5em} $\mc C = \{\}$\\ 
 $\mc S_{1} = \mc X$\\
 $\eta = \beta \frac{\log k + \log(1/\delta)}{(\gamma-1)^4}$\\
 \For{$i = 1$ to $k$}{
 	\vspace{0.7em}\textbf{Phase 1}\\
 	$l = k \eta + 1$\;
	$Z \sim U^l[\mc S_i]$    \mbox{       } //   Draws $l$ independent elements from $\mc S_i$ uniformly at random\\
	For $1 \le t \le i$,\\
             \mbox{            } $Z_t = \{x \in Z : {\mc O}(x)= t\}.$ \mbox{    } //Queries the oracle about the members of $Z$\\
    % let $Z_t \subseteq \mc Z$ be the set of points with label $i$. That is, \begin{center}$A_t = \{x \in \mc Z : x \in C_t\}.$\end{center} 
%	Choose any $Z_p$ such that $|Z_p| > \eta$.\\
$p = \argmax_t |Z_t|$\\
	$\mu_p' := \frac{1}{|Z_p|}\sum_{x \in Z_p} x$.\\

	\vspace{1.5em}\textbf{Phase 2}\\
    // We know that there exists $r_i$ such that $\forall x\in {\mc S_i}$, $x\in C_i \Leftrightarrow d(x, \mu^\prime_i)< r_i$.\\ 
    // Therefore, $r_i$ can be found by simple binary search\\
    
	$\widehat{\mc S_i}$ = Sorted$(\{\mc S_i\})$ \mbox{       }// Sorts elements of $\{x: x\in \mc S_i\}$ in increasing order of $d(x, \mu_p')$.\\    
	 $r_i = $ BinarySearch$(\widehat{\mc S_i})$ \mbox{      } //This step takes up to $O(\log|{\mc S_i}|)$ queries\\


%Binary search over $\widehat{\mc S_i}$, to find an index $idx$ such that $b_{idx} \in C_p$ and $b_{idx+1} \not\in C_p$. (This step involves making queries to the oracle $\mc O$).\\ %We will later prove that $d(b_{idx}, c_p') = \max_{x \in C_p}d(x, c_p')$.
	$C_p' = \{x \in \mc S_i: d(x, \mu_p') \le r_i\}$.\\
	$S_{i+1} = S_{i}\setminus C_p'$.\\
	$\mc C = \mc C \cup \{C_p'\}$
 }
 Output $\mc C$.
 \label{alg:steinerQueryPositive}
 \caption{Algorithm for $\gamma(> 1)$-margin instances with queries}
\end{algorithm}


\begin{lemma}
\label{lemma:hasGammaMargin}
Let $(\mc X, d, C)$ be a clustering instane, where $C$ is center-based and satisfies $\gamma$-margin property. Let $\mu$ be the set of centers corresponding to the centers of mass of $C$. Let $\mu_i'$ be such that $d(\mu_i, \mu_i') \le r(C_i)\epsilon$, where $r(C_i) = \max_{x\in C_i}d(x, \mu_i)$ . Then $\gamma \ge 1 + 2\epsilon$ implies that 

$$\forall x \in C_i, \forall y \in {\mc X} \setminus C_i \Rightarrow d(x, \mu_i') < d(y, \mu_i')$$  
\end{lemma}

%\begin{lemma}
%\label{lemma:hasGammaMargin}
%Given a clustering instance $(\mc X, d)$ in some metric space $M$. Let $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ be a clustering of $\mc X$ induced by centers $\mu_1, \ldots, \mu_k \in M$ which satisfies $\gamma$-margin and $\mu_i$ is the mean of points in $C_i$. Let $\mu_i' \in M$ such that $d(\mu_i, \mu_i') \le r(C_i)\epsilon$. If $\gamma \ge 1 + 2\epsilon$, then for all $x \in C_i$ and for all $y \in C_j$
%$$d(x, \mu_i') < d(y, \mu_i')$$  
%\end{lemma}

\begin{proof}
Fix any $x \in C_i$ and $y \in C_j$. $d(x, \mu_i') \le d(x, \mu_i)+d(\mu_i, \mu_i') \le r(C_i) (1+\epsilon)$. Similarly, $d(y, \mu_i') \ge d(y, \mu_i) - d(\mu_i, \mu_i') > (\gamma -\epsilon)r(C_i)$. Combining the two, we get that $d(x, \mu_i') < \frac{1+\epsilon}{\gamma-\epsilon}d(y, \mu_i')$. 
\qed
\end{proof}

\begin{lemma}
\label{lemma:phase1}
Let the framework be as in Lemma \ref{lemma:hasGammaMargin}. Let $Z_p, C_p, \mu_p$, $\mu_p^\prime$ and $\eta$ be defined as in Algorhtm \ref{alg:steinerQueryPositive}, and $\epsilon = \frac{\gamma - 1}{2}$. If $|Z_p| > \eta$, then the probability that $d(\mu_p, \mu_p^\prime) > r(C_p)\epsilon$ is at most $\frac{\delta}{k}$.
\end{lemma}
\begin{proof}
Define a uniform distribution $U$ over $C_p$. Then $\mu_p$ is the mean of this distribution, and $\mu_p^\prime$ is the empirical mean. Now, we only need to use a concentration inequality to show that the empirical means is close to the true mean. Therefore, using Thm. \ref{thm:genHoeff} from Appendix \ref{appendixsection:conIneq} completes the proof.
\qed
\end{proof}

\begin{theorem}
\label{thm:steinerQueryPositive}
Let $(\mc X, d, C)$ be a clustering instane, where $C$ is center-based and satisfies $\gamma$-margin property. Let $\mu_i$ be the center corresponding to the center of mass of $C_i$.
%Given a clustering instance $(\mc X, d)$ in some metric space $M$. Let $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ be a clustering of $\mc X$ induced by centers $\mu_1, \ldots, \mu_k \in M$ which satisfies $\gamma$-margin and $\mu_i$ is the mean of points in $C_i$. 
Assume $\delta \in (0, 1)$ and $\gamma > 1$. Then with probability at least $1-\delta$, Algorithm \ref{alg:steinerQueryPositive} outputs $C$.
\end{theorem}

\begin{proof}
In the first phase of the algorithm we are making $l>k\eta$ queries. Therefore, using the pigeonhole principle, we know that there exists cluster index $p$ such that $|Z_p| > \eta$. Then Lemma \ref{lemma:phase1} implies that the algorithm chooses a center $\mu_p'$ such that with probability at least $1-\frac{\delta}{k}$ we have $d(\mu_p, \mu_p') \le r(C_p)\epsilon$. By Lemma \ref{lemma:hasGammaMargin}, this would mean that $d(x, \mu_p') < d(y, \mu_p')$ for all $x \in C_p$ and $y \not\in C_p$. Hence, the radius $r_i$ found in the phase two of Alg. \ref{alg:steinerQueryPositive} is such that $r_{i} = \max\limits_{x \in C_p} d(x, \mu_p')$. This implies that $C_p'$ (found in phase two) equals to $C_p$. Hence, with probability at least $1-\frac{\delta}{k}$ one iteration of the algorithm successfully finds all the points in a cluster $C_p$. Using union bound, we get that with probability at least $1-k\frac{\delta}{k} = 1-\delta$, the algorithm recovers the target clustering.
\qed 
\end{proof}

\begin{theorem}
\label{thm:steinerQueryPositiveComplexity}
Let the framework be as in theorem \ref{thm:steinerQueryPositive}. Then Algorithm \ref{alg:steinerQueryPositive} 
\begin{itemize}[nolistsep,noitemsep]
\item Makes $O\big(k^2\log |\mc X| + k^3\frac{\log k + \log (1/\delta)}{(\gamma - 1)^4}\big)$ same-cluster queries to the oracle $\mc O$.
\item Runs in $O\big(k|\mc X|\log |\mc X| + k^2\frac{\log k + \log (1/\delta)}{(\gamma - 1)^4}\big)$ time.
\end{itemize}
\end{theorem}

\begin{proof}
The first phase of the algorithm takes $O(k\eta)$ time and makes $k\eta+1$ cluster-assignment queries to the oracle. The second phase takes $O(n\log n)$ times and makes $O(\log n)$ cluster-assignment queries to the oracle. Hence, in total the algorithm takes $O(kn\log n + k^2\eta)$ time and makes $O(k\log n + k^2\eta)$ cluster-assignment queries. Substituting $\eta = \beta\frac{\log k + \log(1/\delta)}{(\eta-1)^4}$ and using the fact that each cluster-assignment query can be substituted by $k$ same-cluster queries (see appendix \ref{appendix:diffQueryModels}) finishes the proof\footnote{It is actually possible to improve the $k^3$ term in query complexity to $k^2$, by noting that in the second phase of the algorithm we only need to use same-cluster queries rather than cluster-assignment queries.}.
\qed
\end{proof}

\begin{corollary}
The set of Euclidean clustering instances that satisfy $\gamma$-margin property for some $\gamma > 1$ admits query complexity $O\big(k^2\log |\mc X| + k^3\frac{\log k + \log (1/\delta)}{(\gamma - 1)^4}\big)$. 
\begin{proof}
The corollary is the immediate result of theorem \ref{thm:steinerQueryPositiveComplexity} and definition \ref{definition:QueryComplexity}
\end{proof}

\end{corollary}

\section{Lower Bounds}
\label{section:lowerBounds}
In section \ref{section:clusteringWithQuery}, we proposed an algorithm that could recover the true clustering with high probability, assuming that it has the $\gamma$-margin property for $\gamma>1$. A first challenge that comes to mind is that whether we can provide lower bounds for the number required queries.

A more interesting but subtle challenge is revealed by noting that the `clustering with query' model can be contrasted to the usual unsupervised clustering. Assume that the oracle has the optimal $k$-means clustering in its mind. Finding the $k$-means solution without the help of the oracle is generally computationally hard. Therefore, one may ask whether under $\gamma$-margin assumption one can provide a polynomial time algorithm that does not make any queries. In the next section we show a negative answer: for $\gamma < 1.5$ finding the solution to $k$-means clustering is computationally hard. This together with Theorem \ref{thm:steinerQueryPositive} and \ref{thm:steinerQueryPositiveComplexity} shows that for $1<\gamma<1.5$, having access to queries makes the computationally hard problem of $k$-means clustering efficiently solvable\footnote{To be precise, note that the algorithm used for clustering with queries is probabilistic. However, the lower bound that we provide is for deterministic algorithms. However, this means a lower bound for randomized algorithms as well unless $BPP\neq P$}.

Next, we also give a lower bound on the number of queries needed. We show that with making `too few' queries the problem is still hard.

%which for , makes  queries and in polynomial time finds the target clustering with very high probability. In this section, we want to give a lower bound for our problem setting. We will show that if $\gamma < 1.4$ then finding the optimal euclidean $k$-means clustering is NP-Hard without queries. Note that Thms. \ref{thm:steinerQueryPositive} and \ref{thm:steinerQueryPositiveComplexity} shows that we can overcome this lower bound using queries. Next, we also give a lower bound on the number of queries needed. We show that making `too few' (sublogarithmic in $k$) queries also doesn't help and the problem remains NP-Hard.

\subsection{Lower Bound $k$-Means Clustering without Query}
Given a set of instances $\mc X \subset \mb R ^d$, the $k$-means clustering finds a clustering $\mc C = \{C_1, \ldots, C_k\}$ which minimizes the following objective function
$$\sum\limits_{C_i}\frac{1}{|C_i|} \sum_{x, y \in C_i} d^2(x, y).$$
One can also consider a weighted version of the problem where every point $x$ has a weight $w(x)$. 

$$\sum\limits_{C_i}\frac{1}{\sum_{x \in C_i}w(x)} \sum_{x, y \in {C_i \choose 2}} w(x)w(y)d^2(x, y).$$ 

Note that weighted $k$-means is e as Vattani showed \cite{vattani2009hardness} that finding the optimal solution to the weighted $k$-means objective is NP-Hard even in Euclidean plane. However, Vattani's proof does not imply hardness under $\gamma$-margin assumption. We will also use similar ideas and reduction technique to get our lower bound. Therefore, we have to modify the proof by setting up the construction more carefully. Hence, $U$ has a set cover of size $n$ iff $X$ has a clustering of cost $\le L$. We setup the distances such that the optimal clustering has $\gamma$-margin for $\gamma \le 1.5$. This completes the proof of the theorem.  

\begin{theorem}
\label{thm:gammaLower}
For any $\gamma \le 1.5$, finding the optimal solution to the $k$-means objective function is NP-Hard even when the optimal satisfies $\gamma$-margin.
\end{theorem}
To prove the theorem, we will reduce an instance of exact cover by 3-sets (X3C) to the decision version of weighted $k$-means (which asks for a clustering with cost $\le L$). 
\begin{definition}[X3C]
Given a set $U$ containing exactly $3n$ elements and a collection $\mc S = \{S_1, \ldots, S_l\}$ of subsets of $X$ such that each $S_i$ contains exactly three elements. Does there exist $n$ elements in $\mc S$ such that their union is $U$? 
\end{definition}

Before, we go into the details of the proof, we give a brief sketch of the proof of the theorem. The proof constructs a an instance $X$ in the euclidean plane. The instance consists of $l$ different `rows' corresponding to the collection $\mc S$ and roughly $6n$ different `columns' corresponding to the set $U$. Now, the set $X$ and its parameters (distances between the points, number of clusters etc.) are chosen such that in any clustering $C$ of cost $\le L$, the rows can be clustered only in two possible ways. One of them corresponds to the $S_i$ being included in the cover and the other corresponds to it being excluded from the cover. Hence, $U$ has a cover of size $n$ iff $X$ has a clustering of cost $\le L$. We choose the distances such that the optimal clustering satisfies $\gamma$-margin for $\gamma \le 1.5$ thereby completing the proof of the theorem. 
  
\begin{figure}
\input{figures/lowerBdFig}
\caption{This figure is very similar to Fig. 1 in \cite{vattani2009hardness}. The component $H_{l,n}$. The left hand side shows the complete design. Rows $R_i$ have $6n+1$ bullets and two circles. Rows $M_i$ have $3n$ circles. The bullets have weight $w$ while the circles have weight $2w$. The distance between the rows $R_i$ and $M_i$ is $2(h+\sqrt{h^2-1})$. The right side has the distance between the different points. An enlarged view of the points $r_{i, j}$ and $m_{i,j}$ is shown in Fig. \ref{fig:ZFig}}
\label{fig:lowerBoundComponent}
\end{figure}

\subsubsection{Component design}
Given an instance of X3C, that is, given elements $U = \{1, \ldots, 3n\}$ and collection $\mc S$, we construct an instance $X$ of $k$-means as follows. $X = H_{l,n} \cup (\cup_{i=1}^l Z_i)$. The set $X$ is described in Fig. \ref{fig:lowerBoundComponent} and the set $Z$ is described in Fig. \ref{fig:ZFig}.

The component $H_{l,n}$ is as described in left hand side of Fig. \ref{fig:lowerBoundComponent}. The row $R_i$ is composed of $6n + 3$ points $\{s_i, r_{i, 1}, \ldots, r_{i, 6n+1}, f_i\}$. The points $r_{i, j}$ have weight $w$ and points $s_i$ and $f_i$ have weight $2w$. Row $M_i$ is composed of $3n$ points $\{m_{i,1}, \ldots, m_{i, 3n}\}$ of weight $2w$. The distances between the points are shown in Fig. \ref{fig:lowerBoundComponent}.


\begin{figure}
\center
\input{figures/ZFig}
\caption{This figure is very similar to Fig. 2 in \cite{vattani2009hardness}. It describes the points $x_{i,j}$, $x_{i,j}'$, $y_{i,j}$ and $y_{i,j}'$ in the set $Z_i$. The distances between the points are also shown. The points in bullet have weight $w$ while the points in circles have weight $2w$.}
\label{fig:ZFig}
\end{figure}

The set $Z_i$ depends on $S_i$ and is shown in Fig. \ref{fig:setZDescription}.  For every $j\in U$, there are four possible locations $x_{i, j}, x_{i,j'}, y_{i,j}$ and $y_{i, j}'$. One of $x_{i,j}$ and $x_{i,j}'$ will be occupied and one of $y_{i,j}$ and $y_{i,j}'$ will be occupied. Each point has weight $w$. The set $Z_i$ is
\begin{itemize}[nolistsep,noitemsep]
\item $j \in S_i \iff x_{i,j}' \in Z_i$
\item $j \not\in S_i \iff x_{i,j} \in Z_i$
\item $j \in S_{i+1} \iff y_{i,j}' \in Z_i$
\item $j \not\in S_{i+1} \iff y_{i,j} \in Z_i$

\end{itemize}
The parameters of the component are defined below. $$k = (l-1)3n + l(3n+2) \text{ and the cost }L = L_1 + L_2 -n\alpha.$$
$$L_1 = (6n+3)w \text{ and } L_2 = 6n(l-1)\frac{2w\frac{w}{2}}{2w+\frac{w}{2}}h^2 = 6n(l-1)$$
$$h = \sqrt{5}, d = \sqrt{4.5}, \epsilon = \frac{1}{w^2} \text{ and } \alpha = \frac{d}{w}-\frac{1}{2w^3}.$$

\begin{definition}[$A$ and $B$ clustering of $R_i$]
\label{defn:abclusteringVattani}
\noindent $A$ - For $1 \le j \le 3n$, $R_i$ has clusters $\{r_{2j-1}, r_{2j}\}$. Also, it has clusters $\{s_i\}, \{r_{i, 6n+1}, f_i\}$\\
\noindent $B$ - For $1 \le j \le 3n$, $R_i$ has clusters $\{r_{2j}, r_{2j+1}\}$. Also, it has clusters $\{s_i, r_{i, 1}\}, \{f_i\}$
\end{definition}

\begin{definition}[Good point for a cluster]
\label{defn:goodPointVattani}
A cluster $C$ is good for a point $x \not\in C$ if adding $z$ to $C$ increases cost by exactly $\frac{2ww'}{2w+w'}h^2$ 
\end{definition}

Defns. \ref{defn:abclusteringVattani} and \ref{defn:goodPointVattani} have been taken from \cite{vattani2009hardness}. A row grouped in a $A$ clustering costs $(6n+3)w-\alpha$ while a row in $B$ clustering costs $(6n+3)w$. Also, the following simple observations can be made. 
\begin{itemize}[nolistsep,noitemsep]
\item The clusters $\{r_{i,2j-1}, r_{i, 2j}\}$, $\{r_{i,2j}, r_{i, 2j+1}\}$ and $\{m_{i,j}\}$ are good for $x_{i,j}$ and $y_{i-1,j}$.
\item The clusters $\{r_{i,2j}, r_{i, 2j+1}\}$ and $\{m_{i,j}\}$ are good for $x_{i,j}'$ and $y_{i-1,j}'$.
\end{itemize}
Hence, the clustering $C$ with the rows $R_i$ grouped in a $A$ or $B$ clustering and the rows $M_i$ in singleton clusters is a $k$-clustering and costs atmost $L_1$. Adding the points in $Z$ to $C$ in a good way increases the cost by $L_2$. We call such a clustering a {\it nice}-clustering. Hence, a nice clustering has cost atmost $L_1 + L_2$. If $t$ rows are group in a $A$-clustering, the nice-clustering costs $L_1+L_2-t\alpha$.

\subsubsection{Proof}


\begin{lemma}
Any non-nice clustering of $H_{l, n}$ costs atleast $> L_1 + L_2 - l\alpha + w > L + \frac{7}{8}w$ where $w = poly(l, n)$
\end{lemma}

\begin{proof}
Any non-nice $k$-clustering $C$ can take the following form. 
\begin{itemize}[nolistsep]
\item All the clusters have cardinality $\le 3$ - In this case the following scenarios can arise.\\
Contains $m_{i, j}, m_{i,j+1}$ in the same cluster\\
$C$ must have either of the following. (1) $r_{i, j}$ and $r_{i,j+1}$ as a singleton. In this case, the cost difference is atleast $ \frac{141}{6}w - 2w -\frac{20}{6}w > 14w$. (2) $s_i$ and $r_{i,1}$ as singleton. The difference is atleast $13w$. (3) $f_i$ and $r_{i,6n+1}$ as singleton. Similarly,  the cost difference is atleast $13w+\alpha$. (4) $x_{i,j}$ and $y_{i+1,j}$ as singleton. The cost difference is atleast $16w-\frac{20}{3}w > 9w$. Note that having multiple $m_{i, j}$'s in the same cluster or having multiple clusters which contain $m_{i, j}, m_{i,j+1}$ will bring the cost up even furthur.

$C$ contains $r_{i, 2j}, m_{i, j}$ in the some cluster $C_i$\\
This has the following possibilities. (1) $r_{i, j'}$ as a singleton. In this case the smallest cost difference is when $x_{i,j} \in C_i$ which is atleast $ \frac{w}{4}(14+2(\sqrt5+2)^2) - 2w -\frac{10}{3}w > 7w$. (2) $s_i$ and $r_{i,1}$ as singleton. In this case, the In this case the analysis is identical to before. The cost difference is atleast $6w$. (3) $f_i$ and $r_{i,6n+1}$ as singleton. Similar to before, the cost difference is atleast $6w+\alpha$. (4) $x_{i,j}$'s as singleton. In this case, the difference is atleast $ \frac{w}{4}(14+2(\sqrt5+2)^2) + 2w -\frac{10}{3}w > 11w$. Note that having multiple clusters which contain $m_{i, j}, r_{i',j'}$ will bring the cost up even furthur.

Contains $r_{i,j}$ as singleton - $C$ must contain both $\{s_i, r_{i,1}\}$ and $\{r_{i,6n+1}, f_i\}$. Hence the increase in cost is atleast $3w - 2w = w$. We have omitted a few cases for brevity but the analysis is exactly similar. 

\item Contains a cluster of cardinality $m > 4$ - Using a simple counting argument, we can observe that such a clustering costs atleast $\sim 8wm$. A nice clustering however on these points costs atmost $\sim \frac{16w}{6}m$ and uses atmost $m/2$ clusters. Hence, on the $m+m/2$ points, $C$ costs $8wm$ while the nice clustering costs atmost $4mw$. We have omitted the details of the calculations here for brevity. However, note that this follows by observing that in any cluster of size $m > 4$, any point $x$ has atleast $m-4$ at a distance $\ge 4$.
\end{itemize}
\qed
\end{proof}

\begin{lemma}
\label{lemma:kmeansEquivalenceX3C}
The set $X = H_{l,n} \cup Z$ has a $k$-clustering of cost less or equal to $L$ if and only if there is an exact cover for the X3C instance.
\end{lemma}
\begin{proof}
The proof is identical to the proof of Lemma 11 in \cite{vattani2009hardness}.
\end{proof}

\begin{lemma}
\label{lemma:gammaLower}
The clustering instance $X = H_{l,n} \cup Z$ has $\gamma$-margin where $\gamma = 1.5$.
\end{lemma}
\begin{proof}
Any clustering of cost $\le L$ has the following different types of clusters. (1) $\{r_{i,2j-1}, r_{i, 2j}\}$. To this cluster, $x_{i,j}$ or $y_{i,j}$ is added. The radius of this cluster is $\frac{2}{3}h$. The closest distance of any other point to this cluster is atleast $h$. Hence, $\gamma = 1.5$. (2) $\{r_{i,2j}, r_{i, 2j+1}\}$. To this cluster, $x_{i,j}$ or $y_{i,j}$ is added. Identical to 1. 
(3) $\{r_{i,2j}, r_{i, 2j+1}\}$. To this cluster, $x_{i,j}'$ or $y_{i,j}'$ is added. Same as 1 and 2. (4) $\{m_{i,j}\}$. To this cluster, $x_{i,j}$ or $y_{i,j}$ is added. The radius is again $\frac{2}{3}h$. Any other point is atleast $h$ away from the center. Hence, $y= 3/2$. The analysis of all the other cases is analogous.
\qed
\end{proof}
Lemmas \ref{lemma:kmeansEquivalenceX3C} and \ref{lemma:gammaLower} complete the proof of the main result (Thm. \ref{thm:gammaLower}). 

\subsection{Lower Bound on the Number of Queries}

In the previous section we showed that $k$-means clustering is NP-hard even under $\gamma$-margin assumption (for $\gamma < 1.5$). On the other hand, in Section \ref{section:clusteringWithQuery} we showed that this is not the case if the algorithm has access to an oracle. In particular, the algorithm asks $O\big(k^2\log |\mc X| + k^3\frac{\log k + \log (1/\delta)}{(\gamma - 1)^4}\big)$ same-cluster queries\footnote{If the algorithm uses cluster-assignment queries this number reduces by a factor of $k$}. Therefore, a natural follow-up question is that whether there is a polynomial-time algorithm for clustering that uses less number of queries. In this section, we take a first step in this direction and show a lower bound on the number of queries needed to provide a polynomial-time algorithm for $k$-means clustering under margin assumption.

\begin{theorem}
\label{thm:queryLower}
For any $\gamma \le 1.5$, finding the optimal solution to the $k$-means objective function is NP-Hard even when the optimal clustering satisfies $\gamma$-margin and the algorithm can ask $O(\log k + \log |\mc X|)$ same-cluster queries.
\end{theorem}
\begin{proof}
Proof by contradiction: assume that there is polynomial-time algorithm $\mc A$ that makes $O(\log k + \log |\mc X|)$ same-cluster queries to the oracle. Then, we show there exists another algorithm $\mc A^\prime$ for the same problem that is still polynomial but uses no queries. However, this will be a contradiction to Theorem \ref{thm:gammaLower}, which will prove the result.

In order to prove that such $\mc A^\prime$ exists, we use a `simulation' technique. Note that $\mc A$ makes only $q<\beta(\log k + \log |\mc X|)$ binary queries, where $\beta$ is a constant. The oracle therefore can respond to these queries in maximum $2^{q} < k^\beta|\mc X|^\beta$ different ways. Now the algorithm $\mc A^\prime$ can try to simulate all of $k^\beta|\mc X|^\beta$ possible responses by the oracle and output the solution with minimum $k$-means clustering cost. Therefore, $\mc A^\prime$ runs in polynomial-time and is equivalent to $\mc A$.
\end{proof}


\section{Conclusions and Future Directions}
In this work we introduced a framework for clustering with the help of an oracle. Then query complexity and computational complexity of clustering with the help of this oracle was analysed. 

In particular, a notion of niceness of data, i.e., $\gamma$-margin property, was introduced. For $\gamma>1$, a polynomial-time clustering algorithm was proposed that could recover the target clustering with queries (the number of queries is logarithmic in the size of the sample set and polynomial in the number of clusters). This in turns implies that for $\gamma>1$, $k$-means clustering can be performed in polynomial time.

On ther other hand, it was shown that without the use of an oracle, $k$-means clustering is NP-hard for $\gamma < 1.5$. This indicates that for $1<\gamma<1.5$, the use of query is critical, as it makes the computationaly hard problem easy. We also provided lower bounds on the number of queries that can make the hard problem easy, indicating that one would need at least $\omega(kn)$ queries for this to happen.

A possible future direction is to improve the upper bound presented for clustering with queries. While one way of improving this result is to provide an algorithm with better query/computational complexity, the other way is to see whether one can provide an algorithm which works for less restrictive settings, e.g., $\gamma < 1$. Another possible direction is to provide stronger lower bounds for query complexity.

A broader future direction is to use the notion of same-cluster queries in other clustering problems.

\bibliographystyle{alpha}
\bibliography{activeClustering}

\appendix
\section{Relationships Between Query Models}
\label{appendix:diffQueryModels}

\begin{proposition}
Any clustering algorithm that uses only $q$ same-cluster queries can be adjusted to use $2q$ cluster-assignment queries (and no same-cluster queries) with the same order of time complexity.
\end{proposition}
\begin{proof}
We can replace each same-cluster query with two cluster-assignment queries as in $Q(x_1,x_2)={\mathbbm{1}}\{Q(x_1)=Q(x_2))\}$.
\end{proof}

\begin{proposition}
Any algorithm that uses only $q$ cluster-assignment queries can be adjusted to use $kq$ same-cluster queries (and no cluster-assignment queries) with at most a factor $k$ increase in computational complexity, where $k$ is the number of clusters.
\end{proposition}
\begin{proof}
If the clustering algorithm has access to an instance from each of $k$ clusters (say $x_i\in X_i$), then it can simply simulate the cluster-assignment query by making $k$ same-cluster queries ($Q(x) = \argmax_{i}\mathbbm{1}\{Q(x, x_i)\}$). Otherwise, assume that at the time of querying $Q(x)$ it has only instances from $k\prime<k$ clusters. In this case, the algorithm can do the same with the $k\prime$ instances and if it does not find the cluster, assign $x$ to a new cluster index. This will work, because in the clustering task the output of the algorithm is a partition of the elements, and therefore the indices of the clusters do not matter.
\end{proof}


\section{Comparison of $\gamma$-Margin and $\alpha$-Center Proximity}
\label{appendix:gammaMrginVsAlphaCenter}

\begin{definition}[$\alpha$-center proximity \cite{awasthi2012center}]
\label{defn:alphacp}
Given a clustering instance $(\mc X, d)$ in some metric space $M$ and the number of clusters $k$. We say that a clustering $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ induced by centers $c_1, \ldots, c_k \in M$ has $\alpha$-center proximity w.r.t $\mc X$ and $k$ if the following holds. For all $x \in C_i$ and $i\neq j$, 
$$\alpha d(x, c_i) < d(x, c_j)$$
\end{definition}
In this paper, we introduced a notion of $\gamma$-margin. We gave upper (with query) and lower bounds on $\gamma$, when the metric space $M$ is euclidean and the centers are allowed be points in the metric space. Another notion of niceness of clusterability is $\alpha$-center proximity. This notion has been considered in the past in various works \cite{balcan2012clustering,awasthi2012center}. However, the problem setting in these works was different from our current setting. While in the current work, we were focussed on the euclidean metric space, these works considered arbitrary metric space. 

\begin{table}[]
\centering
\caption{Known results for $\alpha$-center proximity}
\label{my-label}
\begin{tabular}{lll}
\cline{2-3}
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{Euclidean} & \multicolumn{1}{l|}{Non-euclidean} \\ \hline
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Centers \\ from data\end{tabular}} & \multicolumn{1}{l|}{?} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Upper bound \cite{balcan2012clustering} - $\sqrt{2}+1$\\ Lower bound \cite{ben2014data} - 2\end{tabular}} \\ \hline
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Centers from\\ metric space\end{tabular}} & \multicolumn{1}{l|}{?} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Upper bound - $2+\sqrt{3}$\\ Lower bound - 3\\ \cite{awasthi2012center}\end{tabular}} \\ \hline
 &  & 
\label{table:alphacenter}
\end{tabular}
\end{table}

An overview of the known results under $\alpha$-center proximity is provided in Table \ref{table:alphacenter}. We will show that using the same techniques as used in the above proofs we can get upper and lower bounds for $\alpha$-center proximity. It is important to note that the upper and lower bounds under $\gamma$-margin are matching. Hence, there is no hope to furthur improve our upper bounds unless P=NP. A summary of our results is provided in \ref{table:gammamargin}.  

\begin{table}[]
\centering
\caption{Results for $\gamma$-margin}
\label{my-label}
\begin{tabular}{lll}
\cline{2-3}
\multicolumn{1}{l|}{}                                                                     & \multicolumn{1}{l|}{Euclidean} & \multicolumn{1}{l|}{Non-euclidean}                                                                         \\ \hline
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Centers \\ from data\end{tabular}}        & \multicolumn{1}{l|}{?}         & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Upper bound (Thm. \ref{thm:upperCenterData}) - 2\\ Lower bound (Thm. \ref{thm:lowerCenterData}) - 2\end{tabular}}           \\ \hline
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Centers from\\ metric space\end{tabular}} & \multicolumn{1}{l|}{}         & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Upper bound (Thm. \ref{thm:upperCenterMetric}) - 3\\ Lower bound (Thm. \ref{thm:lowerCenterMetric}) - 3\\ Awasthi\end{tabular}} \\ \hline
                                                                                          &                       &    
\label{table:gammamargin}                                                                                                                                                                                                 
\end{tabular}
\end{table}

\subsection{Centers from data}
\begin{theorem}
\label{thm:upperCenterData}
Given a clustering instance $(X , d)$. For all $\gamma \ge 2$, Alg. 1 in \cite{balcan2012clustering} outputs a tree $\mc T$ with the following property. For all $k$ and for all $k$-clusterings $\mc C^* = \{C_1^*, \ldots, C_k^* \}$ which satisfy $\gamma$-margin and the cluster centers $\mu_1, \ldots, \mu_k \in X$, the following holds.

For every $1 \le i \le k$, there exists a node $N_i$ in the tree $T$ such that $C_i^* = N_i$. In other words, there exists a pruning of the tree $T$ such that the corresponding clustering equals $C^*$. 
\end{theorem}

\begin{proof}
Let $p, p' \in C_i^*$ and $q \in C_j^*$. \cite{balcan2012clustering} prove the correctness of their algorithm for $\alpha > \sqrt{2} + 1$. Their proof relies only on the following three properties which are implied when $\alpha > \sqrt{2} + 1$. We will show that these properties are implied by $\gamma > 2$ instances as well.
\begin{itemize}[nolistsep,noitemsep]
\item $d(p, \mu_i) < d(p, q)$\\
$\gamma d(p, \mu_i) < d(q, \mu_i) < d(p, q) + d(p, \mu_i) \implies d(p, \mu_i) < \frac{1}{\gamma-1}d(p, q)$.
\item $d(p, \mu_i) < d(q, \mu_i)$\\
This is trivially true since $\gamma > 2$.
\item $d(p, \mu_i) < d(p', q)$\\
Let $r = \max_{x \in C_i^*} d(x, \mu_i)$. Observe that $d(p, \mu_i) < r$. Also, $d(p', q)> d(q, \mu_i)-d(p', \mu_i) > \gamma r - r = (\gamma -1)r$.
\end{itemize}
\qed
\end{proof}

\begin{theorem}
\label{thm:lowerCenterData}
Given a clustering instance $(\mc X, d)$ and the number of clusters $k$. For $\gamma < 2$, finding a clustering which satisfies $\gamma$-margin and where the centers $\mu_1, \ldots, \mu_k \in \mc X$ is NP-Hard.
\end{theorem}
\begin{proof}
\cite{ben2014data} proved that for $\alpha < 2$, finding a clustering which satisfies $\alpha$-margin and where the centers $\mu_1, \ldots, \mu_k \in \mc X$ is NP-Hard. Note that the reduced instance in their proof, also satisfies $\gamma$-margin for $\gamma < 2$. 
\qed
\end{proof}

\subsection{Centers from metric space}
\begin{theorem}
\label{thm:upperCenterMetric}
Given a clustering instance $(X , d)$. For all $\gamma \ge 3$, the standard single-linkage algorithm outputs a tree $\mc T$ with the following property. For all $k$ and for all $k$-clusterings $\mc C^* = \{C_1^*, \ldots, C_k^* \}$ which satisfy $\gamma$-margin, the following holds.

For every $1 \le i \le k$, there exists a node $N_i$ in the tree $T$ such that $C_i^* = N_i$. In other words, there exists a pruning of the tree $T$ such that the corresponding clustering equals $C^*$. 
\end{theorem}

\begin{proof}
\cite{balcan2008discriminative} showed that if a clustering $C^*$ has strong stability property, then single-linkage outputs a tree such that pruning equals $C^*$. It is a simple exercise to see that $\gamma > 3$ instances have strong-stability and the claim follows.  
\qed
\end{proof}


\begin{theorem}
\label{thm:lowerCenterMetric}
Given a clustering instance $(\mc X, d)$ and the number of clusters $k$. For $\gamma < 3$, finding a clustering which satisfies $\gamma$-margin is NP-Hard.
\end{theorem}
\begin{proof}
\cite{awasthi2012center} proved the above claim but for $\alpha < 3$ instances. Note that the reduced instance in their proof, also satisfies $\gamma$-margin for $\gamma < 3$. 
\qed
\end{proof}

\section{Concentration inequalities}
\label{appendixsection:conIneq}

\begin{theorem}[Generalized Hoeffding's Inequality (e.g., \cite{ashtiani2015dimension})]
\label{thm:genHoeff}
Let $X_1, \ldots. X_n$ be i.i.d random vectors in some Hilbert space such that for all $i$, $\|X_i\|_2 \le R$ and $E[X_i] = \mu$. If $n > c\frac{\log(1/\delta)}{\epsilon^2}$, then with probability atleast $1-\delta$, we have that
$$\Big\|\mu - \frac{1}{n}\sum X_i\Big\|_2^2 \le R^2\epsilon$$ 
\end{theorem}

\end{document}


